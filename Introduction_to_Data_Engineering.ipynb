{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3209559f-1c78-48b9-aa16-f4b0e1f80088",
   "metadata": {},
   "source": [
    "# What is Data Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17c5c6-740a-4b3f-8c4b-9b0d2edbb62a",
   "metadata": {},
   "source": [
    "## Modern data ecosystem and role of data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffcf96-7188-448a-a0f2-bf8eeae1daef",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffeb04ff-1762-469f-b4db-f9fc0f4137a7",
   "metadata": {},
   "source": [
    "Modern data ecosystem includes a network of interconnected and continually evolving entities that include:\n",
    "\n",
    "Data, that is available in a host of different formats, structures, and sources. \n",
    "\n",
    "Enterprise Data Environment, in which raw data is staged so it can be organized, cleaned, and optimized for use by end-users.\n",
    "\n",
    "End-users, such as business stakeholders, analysts, and programmers who consume data for various purposes.\n",
    "\n",
    "Emerging technologies such as Cloud Computing, Machine Learning, and Big Data, are continually reshaping the data ecosystem and the possibilities it offers.\n",
    "\n",
    "Data Engineers, Data Analysts, Data Scientists, Business Analysts, and Business Intelligence Analysts, all play a vital role in the ecosystem for deriving insights and business results from data.\n",
    "\n",
    "The goal of Data Engineering is to make quality data available for analytics and decision-making. And it does this by collecting raw source data, processing data so it becomes usable, storing data, and making quality data available to users securely.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40915b74-497a-43da-b22c-9b0e573efcd8",
   "metadata": {},
   "source": [
    "## Responsibilities and Skillsets of a Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d3c27-bb53-4821-98c5-cda6af870c00",
   "metadata": {},
   "source": [
    "### Technical skills"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09e2946a-d8d9-49ad-92a2-b3f9ef516020",
   "metadata": {},
   "source": [
    "Knowledge of working with operating systems such as UNIX, Linux, and Windows, including commonly used administrative tools, system utilities and commands.\n",
    "Knowledge of infrastructure components, such as virtual machines, networking, and application services, such as load balancing and application performance monitoring.\n",
    "Also, cloud-based services such as those offered by Amazon, Google, IBM, and Microsoft.\n",
    "Experience of working with databases and data warehouses, which include: RDBMSes such as IBM DB2, MySQL, Oracle Database, and PostgreSQL. NoSQL databases such as Redis, MongoDB, Cassandra, and Neo4J. Data warehouses such as Oracle Exadata, IBM Db2 Warehouse on Cloud, IBM Netezza Performance Server, and Amazon RedShift.\n",
    "A high-level of proficiency working with data pipelines. Popular data pipeline solutions include Apache Beam, AirFlow, and DataFLow.\n",
    "Experience of working with ETL tools such as IBM Infosphere Information Server, AWS Glue, and Improvado.\n",
    "Proficiency in languages for querying, manipulating, and processing data. This includes: Query languages for accessing and manipulating data in a database, such as SQL for relational databases and SQL-like query languages for NoSQL databases.\n",
    "Programming languages such as Python, R, and Java. Shell and Scripting languages, such as Unix/Linux Shell and PowerShell.\n",
    "Familiarity with Big Data processing tools such as Hadoop, Hive, and Spark.\n",
    "##\n",
    "Kafka is part of the data pipeline ecosystem, but it's not an ETL tool or orchestrator. It's best described as a real-time event streaming platform that enables pipelines, feeds ETL tools, and supports orchestration.\n",
    "##\n",
    "Kafka, Spark, Storm are applications for processing data streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6adf4c-55a0-4d3e-99e9-a5accb6db592",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ebcc8c3-14ce-4cd5-a44f-2327b24f0c04",
   "metadata": {},
   "source": [
    "The role of a Data Engineer includes:\n",
    "\n",
    "Gathering data from disparate sources.\n",
    "\n",
    "Integrating data into a unified view for data consumers.\n",
    "\n",
    "Preparing data for analytics and reporting.\n",
    "\n",
    "Managing data pipelines for a continuous flow of data from source to destination systems.\n",
    "\n",
    "Managing the complete infrastructure for the collection, processing, and storage of data.\n",
    "\n",
    "To be successful in their role, Data Engineers need a mix of technical, functional, and soft skills.\n",
    "\n",
    "Technical Skills include working with different operating systems and infrastructure components such as virtual machines, networks, and application services. It also includes working with databases and data warehouses, data pipelines, ETL tools, big data processing tools, and languages for querying, manipulating, and processing data. \n",
    "\n",
    "An understanding of the potential application of data in business is an important skill for a data engineer. Other functional skills include the ability to convert business requirements into technical specifications, an understanding of the software development lifecycle, and the areas of data quality, privacy, security, and governance. \n",
    "\n",
    "Soft Skills include interpersonal skills, the ability to work collaboratively, teamwork, and effective communication. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbacbd7-3b7c-4427-9cbe-e550b2e9c0f1",
   "metadata": {},
   "source": [
    "## The Data Ecosystem and Languages for Data Professionals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab35183-9375-424b-8104-ffe409516c44",
   "metadata": {},
   "source": [
    "### Metadata and Metadata Management"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb94692d-e97d-4fc8-a485-0711d38857d7",
   "metadata": {},
   "source": [
    "Objectives\n",
    "After completing this reading, you will be able to:\n",
    "\n",
    "Define what metadata is\n",
    "Describe what metadata management is\n",
    "Explain the importance of metadata management\n",
    "List popular tools for metadata management\n",
    "What is metadata?\n",
    "Metadata is data that provides information about other data.\n",
    "\n",
    "This is a very broad definition. Here we will consider the concept of metadata within the context of databases, data warehousing, business intelligence systems, and all kinds of data repositories and platforms.\n",
    "\n",
    "We'll consider the following three main types of metadata:\n",
    "\n",
    "Technical metadata\n",
    "Process metadata, and\n",
    "Business metadata\n",
    "Technical metadata\n",
    "Technical metadata is metadata which defines the data structures in data repositories or platforms, primarily from a technical perspective.\n",
    "\n",
    "For example, technical metadata in a data warehouse includes assets such as:\n",
    "\n",
    "Tables that record information about the tables stored in a database, like:\n",
    "\n",
    "each table's name\n",
    "the number of columns and rows each table has\n",
    "A data catalog, which is an inventory of tables that contain information, like:\n",
    "\n",
    "the name of each database in the enterprise data warehouse\n",
    "the name of each column present in each database\n",
    "the names of every table that each column is contained in\n",
    "the type of data that each column contains\n",
    "The technical metadata for relational databases is typically stored in specialized tables in the database called the System Catalog.\n",
    "\n",
    "Process metadata\n",
    "Process metadata describes the processes that operate behind business systems such as data warehouses, accounting systems, or customer relationship management tools.\n",
    "\n",
    "Many important enterprise systems are responsible for collecting and processing data from various sources. Such critical systems need to be monitored for failures and any performance anomalies that arise. Process metadata for such systems includes tracking things like:\n",
    "\n",
    "process start and end times\n",
    "disk usage\n",
    "where data was moved from and to, and\n",
    "how many users access the system at any given time\n",
    "This sort of data is invaluable for troubleshooting and optimizing workflows and ad hoc queries.\n",
    "\n",
    "Business metadata\n",
    "Users who want to explore and analyze data within and outside the enterprise are typically interested in data discovery. They need to be able to find data which is meaningful and valuable to them and know where that data can be accessed from. These business-minded users are thus interested in business metadata, which is information about the data described in readily interpretable ways, such as:\n",
    "\n",
    "how the data is acquired\n",
    "what the data is measuring or describing\n",
    "the connection between the data and other data sources\n",
    "Business metadata also serves as documentation for the entire data warehouse system.\n",
    "\n",
    "Managing metadata\n",
    "Managing metadata includes developing and administering policies and processes to ensure information can be accessed and integrated from various sources and appropriately shared across the entire enterprise.\n",
    "\n",
    "Creation of a reliable, user-friendly data catalog is a primary objective of a metadata management model. The data catalog is a core component of a modern metadata management system, serving as the main asset around which metadata management is administered. It serves as the basis by which companies can inventory and efficiently organize their data systems. A modern metadata management model will include a web-based user interface that enables engineers and business users to easily search for and find information on key attributes such as CustomerName or ProductType. This kind of model is central to any Data Governance initiative.\n",
    "\n",
    "Why is metadata management important?\n",
    "Good metadata management has many valuable benefits. Having access to a well implemented data catalog greatly enhances data discovery, repeatability, governance, and can also facilitate access to data.\n",
    "\n",
    "Well managed metadata helps you to understand both the business context associated with the enterprise data and the data lineage, which helps to improve data governance. Data lineage provides information about the origin of the data and how it gets transformed and moved, and thus it facilitates tracing of data errors back to their root cause. Data governance is a data management concept concerning the capability that enables an organization to ensure that high data quality exists throughout the complete lifecycle of the data, and data controls are implemented that support business objectives.\n",
    "\n",
    "The key focus areas of data governance include availability, usability, consistency, data integrity and data security and includes establishing processes to ensure effective data management throughout the enterprise such as accountability for the adverse effects of poor data quality and ensuring that the data which an enterprise has can be used by the entire organization.\n",
    "\n",
    "Popular tools for metadata management\n",
    "Popular metadata management tools include:\n",
    "\n",
    "IBM InfoSphere Information Server\n",
    "CA Erwin Data Modeler\n",
    "Oracle Warehouse Builder\n",
    "SAS Data Integration Server\n",
    "Talend Data Fabric\n",
    "Alation Data Catalog\n",
    "SAP Information Steward\n",
    "Microsoft Azure Data Catalog\n",
    "IBM Watson Knowledge Catalog\n",
    "Oracle Enterprise Metadata Management (OEMM)\n",
    "Adaptive Metadata Manager\n",
    "Unifi Data Catalog\n",
    "data.world\n",
    "Informatica Enterprise Data Catalog\n",
    "Summary\n",
    "In this reading, you learned that:\n",
    "\n",
    "Metadata is data that provides information about other data, and includes three main types: technical, process, and business metadata\n",
    "The technical metadata for relational databases is typically stored in specialized tables in the database called the system catalog\n",
    "A primary objective of business metadata management modelling is the creation and maintenance of a reliable, user-friendly data catalog\n",
    "Having access to a well-implemented data catalog greatly enhances data discovery, repeatability, governance, and can also facilitate access to data\n",
    "Metadata management tools from IBM include InfoSphere Information Server and Watson Knowledge Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdcb0f-0314-4970-a716-cbe1ce510f5c",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9f40ed1-e0a1-4f73-b3c1-3542db695bc5",
   "metadata": {},
   "source": [
    "Summary and Highlights\n",
    "In this lesson, you have learned:\n",
    "\n",
    "A Data Engineer’s ecosystem includes the infrastructure, tools, frameworks, and processes for extracting data, architecting and managing data pipelines and data repositories, managing workflows, developing applications, and managing BI and Reporting tools.\n",
    "\n",
    "Based on how well-defined the structure of the data is, data can be categorized as\n",
    "\n",
    "Structured data, that is data which is well organized in formats that can be stored in databases.\n",
    "\n",
    "Semi-structured data, that is data which is partially organized and partially free-form.\n",
    "\n",
    "Unstructured data, that is data which can not be organized conventionally into rows and columns.\n",
    "\n",
    "Data comes in a wide-ranging variety of file formats, such as, delimited text files, spreadsheets, XML, PDF, and JSON, each with its own list of benefits and limitations of use.\n",
    "\n",
    "Data is extracted from multiple data sources, ranging from relational and non-relational databases, to APIs, web services, data streams, social platforms, and sensor devices.\n",
    "\n",
    "Once the data is identified and gathered from different sources, it needs to be staged in a data repository so that it can be prepared for analysis. The type, format, and sources of data influence the type of data repository that can be used.\n",
    "\n",
    "Data professionals need a host of languages that can help them extract, prepare, and analyse data. These can be classified as:\n",
    "\n",
    "Querying languages, such as SQL, used for accessing and manipulating data from databases.\n",
    "\n",
    "Programming languages such as Python, R, and Java, for developing applications and controlling application behavior.\n",
    "\n",
    "Shell and Scripting languages, such as Unix/Linux Shell, and PowerShell, for automating repetitive operational tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b032e10-79ed-4cb6-ad60-409b05f0fce5",
   "metadata": {},
   "source": [
    "## Data Repositories, Data Pipelines, and Data Integration Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a9cf2-c3b8-4e2e-8c35-017f587113d9",
   "metadata": {},
   "source": [
    "### NoSQL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce029bec-06e4-421a-bc2b-4571d0175c3a",
   "metadata": {},
   "source": [
    "NoSQL, which stands for “not only SQL,”\n",
    "or sometimes “non SQL” is a non-relational database design that provides flexible schemas\n",
    "for the storage and retrieval of data. NoSQL databases have existed for many years\n",
    "but have only recently become more popular in the era of cloud, big data, and high-volume\n",
    "web and mobile applications. They are chosen today for their attributes\n",
    "around scale, performance,and ease of use. It's important to emphasize that the \"No\"\n",
    "in \"NoSQL\" is an abbreviation for \"not only\" and not the actual word \"No.\" NoSQL databases are built for specific data\n",
    "models and have flexible schemas that allow programmers to create and manage modern applications. They do not use a traditional row/column/table\n",
    "database design with fixed schemas, and typically not use the structured query language (or\n",
    "SQL) to query data, although some may support SQL or SQL-like interfaces. NoSQLallows data to be stored in a schema-less\n",
    "or free-form fashion. Any data, be It structured, semi-structured,\n",
    "or unstructured,can be stored in any record. Based on the model being used for storing\n",
    "data, there are four common types of NoSQL databases: Key-value store, Document-based, Column-based, and graph-based. Key-value store: Data in a key-value database\n",
    "is stored as a collection of key-value pairs. The key represents an attribute of the data\n",
    "and is a unique identifier. Both keys and values can be anything from\n",
    "simple integers or strings to complex JSON documents. Key-value stores are great for storing user\n",
    "session dataanduser preferences, making real-time recommendations and targeted advertising,\n",
    "and in-memory data caching. However, if you want to be able to query the\n",
    "data on specific data value, need relationships between data values, or need to have multiple\n",
    "unique keys, a key-value store may not be the best fit. Redis, Memcached, and DynamoDB are some well-known\n",
    "examples in this category. Document-based: Document databasesstore each\n",
    "record and its associated data within a single document. They enable flexible indexing, powerful ad\n",
    "hoc queries, and analytics over collections of documents. Document databases are preferable for eCommerce\n",
    "platforms, medical records storage, CRM platforms, and analytics platforms. However, if you’re looking to run complex\n",
    "search queries and multi-operation transactions, a document-based database may not be the best\n",
    "option for you. MongoDB, DocumentDB, CouchDB, and Cloudant\n",
    "are some of the popular document-based databases. Column-based: Column-based models store data\n",
    "in cells grouped as columns of data instead of rows. A logical grouping of columns, that is, columns\n",
    "that are usually accessed together, is called a column family. For example, a customer’s name and profile\n",
    "information will most likely be accessed together but not their purchase history. So,customer name and profile information data\n",
    "can be grouped into a column family. Since column databases store all cells corresponding\n",
    "to a column as a continuous disk entry, accessing and searching the data becomes very fast. Column databases can be great for systems\n",
    "that require heavy write requests, storing time-series data, weather data, and IoT data. But if you need to use complex queries or\n",
    "change your querying patterns frequently, this may not be the best option for you. The most popular column databases are Cassandra\n",
    "and HBase. Graph-based: Graph-based databases use a graphical\n",
    "model to represent and store data. They are particularly useful for visualizing,\n",
    "analyzing, and finding connections between different pieces of data. The circles arenodes, and they contain the\n",
    "data. The arrows represent relationships. Graph databases are an excellent choice for\n",
    "working with connected data, which is data that contains lots of interconnected relationships. Graph databases are great for social networks,\n",
    "real-time product recommendations, network diagrams, fraud detection, and access management. But if you want to process high volumes of\n",
    "transactions, it may not be the best choice for you, because graph databases are not optimized\n",
    "for large-volume analytics queries. Neo4J and CosmosDB are some of the more popular\n",
    "graph databases. Advantages of NoSQLNoSQL was created in response\n",
    "to the limitations of traditional relational database technology. The primary advantage of NoSQL is its ability\n",
    "to handle large volumes of structured, semi-structured, and unstructured data. Some of its other advantages include: The ability to run as distributed systemsscaled\n",
    "across multiple data centers, which enables them to take advantage of cloud computing\n",
    "infrastructure; An efficient and cost-effective scale-out\n",
    "architecture that provides additional capacity and performance with the addition of new nodes;\n",
    "and Simpler design, better control over availability,\n",
    "and improved scalability that enables you to be more agile, more flexible, and to iterate\n",
    "more quickly To summarizethe key differencesbetween relational\n",
    "and non-relational databases: RDBMS schemas rigidly define how all data\n",
    "inserted into the database must be typed and composed, whereas NoSQL databases can be schema-agnostic,\n",
    "allowing unstructured and semi-structured data to be stored and manipulated. Maintaining high-end, commercial relational\n",
    "database management systems is expensive whereas NoSQL databases are specifically designed\n",
    "for low-cost commodity hardware Relational databases, unlike most NoSQL, support\n",
    "ACID-compliance, which ensures reliability of transactions and crash recovery. RDBMS is a mature and well-documented technology,\n",
    "which means the risks are more or less perceivable as compared to NoSQL, which is a relatively\n",
    "newer technology. Nonetheless, NoSQL databases are here to stay,\n",
    "and are increasingly being used for mission critical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54e785-11b7-4bad-b4e3-e5a2bfaf1d48",
   "metadata": {},
   "source": [
    "### Data Warehouses, Data Marts, and Data Lakes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f2afd7c-4a93-480a-a9b8-1f6c87add020",
   "metadata": {},
   "source": [
    "All data mining repositories have a similar\n",
    "goal, which is to house data for reporting, analysis, and deriving insights. But their purpose, types of data stored, and\n",
    "how data is accessed differs. In this video, we will learn about some of\n",
    "the characteristics and applications of data warehouses, data marts, and data lakes. A data warehouse is a central repository of\n",
    "data integrated from multiple sources. Data warehouses serve as the single source\n",
    "of truth—storing current and historical data that has been cleansed, conformed, and\n",
    "categorized. When data gets loaded into the data warehouse,\n",
    "it is already modeled and structured for a specific purpose, meaning it's analysis-ready. Traditionally, data warehouses are known to\n",
    "store relational data from transactional systems and operational databases such as CRM, ERP,\n",
    "HR, and Finance applications. But with the emergence of NoSQL technologies\n",
    "and new data sources, non-relational data repositories are also being used for data\n",
    "warehousing. Typically, a data warehouse has a three-tier\n",
    "architecture: The bottom tier of the architecture includes\n",
    "the database servers, which could be relational, non-relational, or both, that extract data\n",
    "from different sources. The middle tier of the architecture consists\n",
    "of the OLAP Server, a category of software that allows users to process and analyze information\n",
    "coming from multiple database servers. And the topmost tier of the architecture includes\n",
    "the client front-end layer. This tier includes all the tools and applications\n",
    "used for querying, reporting, and analyzing data. In response to the rapid data growth and today's\n",
    "sophisticated analytics tools, data warehouses that once resided in on-premise data centers\n",
    "are moving to the cloud. Compared to their on-premise versions, some\n",
    "of the benefits offered by cloud-based data warehouses include: Lower costs, Limitless storage and compute capabilities, Scale on a pay-as-you-go basis; and Faster disaster recovery. As an organization, you would opt for a data\n",
    "warehouse when you have massive amounts of data from your operational systems that need\n",
    "to be readily available for reporting and analysis. Some of the popularly used data warehouses\n",
    "include Teradata Enterprise Data Warehouse platform, Oracle Exadata, IBM Db2 Warehouse\n",
    "on Cloud, IBM Netezza Performance Server, Amazon RedShift, BigQuery by Google Cloudera's\n",
    "Enterprise Data Hub, and Snowflake Cloud Data Warehouse. A data mart is a sub-section of the data warehouse,\n",
    "built specifically for a particular business function, purpose, or community of users. For example, sales or finance groups in an\n",
    "organization accessing data for their quarterly reporting and projections. There are three basic types of data marts—dependent,\n",
    "independent, and hybrid data marts. Dependent data marts are a sub-section of\n",
    "an enterprise data warehouse. Since a dependent data mart offers analytical\n",
    "capabilities for a restricted area of the data warehouse, it also provides isolated\n",
    "security and isolated performance. Independent data marts are created from sources\n",
    "other than an enterprise data warehouse, such as internal operational systems or external\n",
    "data. Hybrid data marts combine inputs from data\n",
    "warehouses, operational systems, and external systems. The difference also lies in how data is extracted\n",
    "from the source systems, the transformations that need to be applied, and how the data\n",
    "is transported into the mart. Dependent data marts, for example, pull data\n",
    "from an enterprise data warehouse, where data has already been cleaned and transformed. Independent data marts need to carry out the\n",
    "transformation process on the source data since it is coming directly from operational\n",
    "systems and external sources. Whatever the type, the purpose of a data mart\n",
    "is to: provide users' data that is most relevant\n",
    "to them when they need it, accelerate business processes by providing\n",
    "efficient response times, provide a cost and time-efficient way in which\n",
    "data-driven decisions can be taken, improve end-user response time; and provide secure access and control. A Data Lake is a data repository that can\n",
    "store large amounts of structured, semi-structured, and unstructured data in their native format. While a data warehouse stores data that has\n",
    "been cleaned, processed, and transformed for a specific need, you do not need to define\n",
    "the structure and schema of data before loading into the data lake. You do not even need to know all of the use\n",
    "cases for which you will ultimately be analyzing the data. A data lake exists as a repository of raw\n",
    "data in its native format, straight from the source, to be transformed based on the use\n",
    "case for which it needs to be analyzed. Which does not mean that a data lake is a\n",
    "place where data can be dumped without governance. While in the data lake, the data is appropriately\n",
    "classified, protected, and governed. A data lake is a reference architecture that\n",
    "is independent of technology. Data lakes combine a variety of technologies\n",
    "that come together to facilitate agile data exploration for analysts and data scientists. Data lakes can be deployed using Cloud Object\n",
    "Storage, such as Amazon S3, or large-scale distributed systems such as Apache Hadoop,\n",
    "used for processing Big Data. They can also be deployed on different relational\n",
    "database management systems, as well as NoSQL data repositories that can store very large\n",
    "amounts of data. Data lakes offer a number of benefits, such\n",
    "as: The ability to store all types of data – unstructured\n",
    "data such as documents, emails, PDFs, semi-structured data such as JSON, XML, CSV, and logs, as\n",
    "well as structured data from relational databases The agility to scale based on storage capacity\n",
    "– growing from terabytes to petabytes of data Saving time in defining structures, schemas,\n",
    "and transformations since data is imported in its original format and The ability to repurpose data in several different\n",
    "ways and wide-ranging use cases. This is extremely beneficial as it is hard\n",
    "for businesses to foresee all the different ways in which you could potentially leverage\n",
    "their data in the future. Some of the vendors that provide technologies,\n",
    "platforms, and reference architectures for data lakes include Amazon, Cloudera, Google,\n",
    "IBM, Informatica, Microsoft, Oracle, SAS, Snowflake, Teradata, and Zaloni. In this video, we learned about some of the\n",
    "capabilities of data mining repositories such as data warehouses, data marts, and data lakes. While they all have a similar goal, they need\n",
    "to be evaluated within the context of the use case and technology infrastructure for\n",
    "selecting the one that works best for an organization’s needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d49c7-714f-4a77-9669-6de8f6b39348",
   "metadata": {},
   "source": [
    "### Data Lakehouses Explained"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0cb8554-a756-4834-905c-2f240f56ed48",
   "metadata": {},
   "source": [
    "So last week, I'm having dinner at this\n",
    "restaurant and I'm looking around, the place is packed,\n",
    "everyone's getting their orders on time. And I couldn't help but\n",
    "think about the logistics that go into a restaurant turning raw\n",
    "ingredients into these delicious meals. So let's think about this for a minute. So, in a commercial kitchen, we have raw ingredients being delivered by trucks to our loading dock on large pallets, right? So truck comes in to the loading dock,\n",
    "they drop off the pallet, and the truck is back out on the road to deliver more\n",
    "ingredients to other restaurants. So that's the easy part. Now we actually have to unwrap\n",
    "this pallet and process it, right? We have to sort everything on it. We have to label all of our ingredients,\n",
    "right? And then we also have to make sure that\n",
    "each item is routed to the correct storage area. So these things could be going\n",
    "into a pantry for dry goods, or it could also be going into\n",
    "large walk in fridges and freezers for\n",
    "things like fresh vegetables and meats. And we also have to organize\n",
    "those storage areas, right? So we've got to make sure that ingredients\n",
    "that are expiring first are used first. We've got to make sure certain ingredients\n",
    "are separated from one another for contamination reasons. And we also have to make sure that\n",
    "certain ingredients hit a very certain temperature also for food safety. And by the way, we need to do all of\n",
    "this as quickly as possible, right? To minimize things like food waste, to minimize spoilage that we\n",
    "could see from the ingredients just sitting on the truck or\n",
    "on a pallet, right? And without this process,\n",
    "the cooks in the kitchen can't really do their job\n",
    "as effectively or safely. They'd be spending a lot of their\n",
    "time just looking for ingredients and less time actually cooking and serving\n",
    "out meals to their customers, right? Okay, so\n",
    "what does this have to do with data? Well, if we think about it,\n",
    "this very same process also exists within data architectures\n",
    "of organizations. So you've got all sorts of different\n",
    "data coming into your organizations from different sources,\n",
    "such as in different cloud environments, different operational applications. Now we even have social media data, right? All this is coming into our\n",
    "organization just like a kitchen has ingredients coming from\n",
    "different suppliers, okay? So constantly have data coming in. We need a quick place to dump all\n",
    "different types of data in different formats for later use. So we have data lakes. Now, these lakes allow us to cheaply and quickly capture raw, structured, and unstructured and\n",
    "even semi structured data. Okay, so now, just like in the kitchen, we're not really cooking\n",
    "on the loading dock, right? Now, maybe I can put a tiny grill\n",
    "there if I really wanted to. But we have to organize and transform this data from its raw state\n",
    "into something that's usable for the kind of insights and analytics\n",
    "that our business wants to generate. So we have enterprise data warehouses or\n",
    "EDWs, right, where data is loaded in,\n",
    "sometimes from a data lake, but sometimes from other sources\n",
    "like operational applications. And it's optimized and organized to run very specific analytical tasks. Now, this could be powering\n",
    "different business intelligence or BI workloads such as building\n",
    "dashboards and reports, or it could be feeding into\n",
    "other analytical tools. Just like our pantries and\n",
    "freezers data in the warehouse is cleaned, organized, governed, and\n",
    "should be trusted for integrity. Okay, so what are some of the challenges\n",
    "that we see in this approach? Well, as we said, data lakes are really awesome to capture\n",
    "tons of data in a cost effective way. But we run into challenges with data governance and data quality, right? And a lot of times these data\n",
    "lakes can become data swamps. And this happens when there's a lot of\n",
    "duplicate, inaccurate, or incomplete data, making it difficult to track and\n",
    "manage assets. So if you think about it,\n",
    "what happens when that data becomes stale? Well, it loses its value in creating\n",
    "insights, the same way that ingredients go bad over time in our restaurant\n",
    "if we don't use them. So data lakes also have challenges\n",
    "with query performance. Since they're not built and optimized to\n",
    "handle the complex analytical queries, it can sometimes be tough to get\n",
    "insights out of lakes directly. Okay, so let's take a look\n",
    "at the data warehouse now. Now, these are really great\n",
    "at query performance. They're exceptional, but\n",
    "they can come at a high cost, right? Just like those big freezers\n",
    "can be very costly to run, we can't put everything\n",
    "into a data warehouse. Now, they can be better optimized\n",
    "to maintain data governance and quality, but they have limited support for semi structured and\n",
    "unstructured data sources. By the way, the ones that are growing the most\n",
    "that are coming into our organization. And they can also sometimes be\n",
    "too slow for certain types of applications that require the freshest\n",
    "data because it takes time to sort, clean, and load data into the warehouse. Okay, so what do we do here? Well, developers took a step back and\n",
    "said, hey, let's take the best of both data lakes and\n",
    "data warehouses and combine them into a new technology\n",
    "called the data lake house. So we get the flexibility and we get the cost effectiveness\n",
    "of a data lake, and we get the performance and structure of a data warehouse. So we'll talk more specifically about\n",
    "the architecture of a data lake house in a future video. But from a value point of view,\n",
    "the lake house lets us store data from the exploding number of new\n",
    "sources in a low cost way, and then leverages built\n",
    "in data management and governance layers to allow us to\n",
    "power both business intelligence and high performance machine\n",
    "learning workloads quickly. Okay, so there are plenty of ways\n",
    "that we can start using a lakehouse. We can modernize our existing data lakes,\n",
    "we can complement our data warehouses to support some of these new types of AI and\n",
    "machine learning driven workloads, but we'll also talk about\n",
    "that in the future video. So the next time you're at a restaurant, I hope you think about how the meal\n",
    "on your plate got there and the steps the ingredients took to go from\n",
    "the kitchen to the meal on your plate. Thank you. If you like this video and want to see\n",
    "more like it, please like and subscribe. If you have questions,\n",
    "please drop them in the comments below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0817a-9fa4-4937-99d2-26740c5024cf",
   "metadata": {},
   "source": [
    "### ETL, ELT, and Data Pipelines"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edec1880-5a4d-4a1c-a04a-23d416ace585",
   "metadata": {},
   "source": [
    "In this video, we will learn about some of\n",
    "the different tools and processes that work to move data from source to destination systems,\n",
    "such as: ETL, or the Extract, Transform, and Load Process ELT, or the Extract, Load, and Transform Process;\n",
    "and Data Pipelines Now we come to the process that is at the\n",
    "heart of gaining value from data. The Extract, Transform, and Load process,\n",
    "or ETL. ETL is how raw data is converted into analysis-ready\n",
    "data. It is an automated process in which you gather\n",
    "raw data from identified sources, extract the information that aligns with your\n",
    "reporting and analysis needs, clean, standardize, and transform that data\n",
    "into a format that is usable in the context of your organization; and load it into a data repository While ETL is a generic process, the actual\n",
    "job can be very different in usage, utility, and complexity. Extract is the step where data from source\n",
    "locations is collected for transformation. Data extraction could be through Batch processing, meaning source data is moved\n",
    "in large chunks from the source to the target system at scheduled intervals. Tools for batch processing include Stitch\n",
    "and Blendo. Stream processing, which means source data\n",
    "is pulled in real-time from the source and transformed while it is in transit and before\n",
    "it is loaded into the data repository. Tools for stream processing include Apache\n",
    "Samza, Apache Storm, and Apache Kafka. Transform involves the execution of rules\n",
    "and functions that convert raw data into data that can be used for analysis. For example, making date formats and units of measurement\n",
    "consistent across all source data removing duplicate data filtering out data that you do not need enriching data, for example, splitting full\n",
    "name to first, middle, and last names establishing key relationships across tables applying business rules and data validations Load is the step where processed data is transported\n",
    "to a destination system or data repository. It could be: Initial loading, that is, populating all the\n",
    "data in the repository; Incremental loading, that is, applying ongoing\n",
    "updates and modifications as needed periodically; or Full refresh, that is, erasing contents of\n",
    "one or more tables and reloading with fresh data Load verification—which includes data checks\n",
    "for missing or null values, server performance, and monitoring load failures, are important\n",
    "parts of this process step. It is vital to keep an eye on load failures\n",
    "and ensure the right recovery mechanisms are in place. ETL has historically been used for batch workloads\n",
    "on a large scale. However, with the emergence of streaming ETL\n",
    "tools, they are increasingly being used for real-time streaming event data as well. Some of the popular ETL tools available include\n",
    "IBM Infosphere Information Server, AWS Glue, Improvado, Skyvia, HEVO, and Informatica PowerCenter. Now let’s look at a variation of the ETL\n",
    "process—the Extract, Load, and Transfer, or ELT process. In the ELT process, extracted data is first\n",
    "loaded into the target system, and transformations are applied in the target system. The destination system for an ELT pipeline\n",
    "is most likely a data lake, though it can also be a data warehouse. ELT is a relatively new technology powered\n",
    "by cloud technologies. Why do we need an ELT process? ELT is useful for processing large sets of\n",
    "unstructured and non-relational data. It is ideal for data lakes where transformations\n",
    "on the data are applied once the raw data is loaded into the data lake. The ELT process has several advantages. Since the raw data is delivered directly to\n",
    "the destination system rather than a staging environment, this shortens the cycle between\n",
    "extraction and delivery. ELT paired with a data lake allows you to\n",
    "ingest volumes of raw data as immediately as the data becomes available. Compared to the ETL process, ELT affords greater\n",
    "flexibility to analysts and data scientists for exploratory data analytics. ELT transforms only that data which is required\n",
    "for a particular analysis so it can be leveraged for multiple use cases. In the ETL process, the entire structure of\n",
    "the data in the warehouse may need to be modified if it is not suited for a new use case. ELT is more suited to work with Big Data. It’s common to see the terms ETL or ELT\n",
    "and data pipelines used interchangeably. And although both move data from source to\n",
    "destination, data pipeline is a broader term that encompasses the entire journey of moving\n",
    "data from one system to another, of which ETL and ELT may be subsets. Data pipelines can be architected for batch\n",
    "processing, for streaming data, and a combination of batch and streaming data. In the case of streaming data, data processing\n",
    "or transformation, happens in a continuous flow. This is particularly useful for data that\n",
    "needs constant updating, such as data from a sensor monitoring traffic. A data pipeline is a high performing system\n",
    "that supports both long-running batch queries and smaller interactive queries. The destination for a data pipeline is typically\n",
    "a data lake, although data may also be loaded to different target destinations, such as\n",
    "another application or a visualization tool. There are a number of data pipeline solutions\n",
    "available, most popular among them being Apache Beam, AirFlow, and DataFlow. In this video, we learned about some of the\n",
    "different data movement approaches—the ETL (or Extract, Transfer, and Load process) and\n",
    "ELT (or the Extract, Load, and Transform process). We also learned about Data Pipelines, encompassing\n",
    "the complete journey of data from one system to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e37e8-97cb-4c64-8155-6d545b95c85d",
   "metadata": {},
   "source": [
    "### Data Integration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff73a12b-b282-40c2-bdf7-0d19a8d5fd5f",
   "metadata": {},
   "source": [
    "Gartner defines data integration as a discipline\n",
    "comprising the practices, architectural techniques, and tools that allow organizations to ingest,\n",
    "transform, combine, and provision data across various data types. The report further explains that data integration\n",
    "has several usage scenarios, such as data consistency across applications, master data\n",
    "management, data sharing between enterprises, and data migration and consolidation. In the field of analytics and data science,\n",
    "data integration includes accessing, queueing, or extracting data from\n",
    "operational systems transforming and merging extracted data either\n",
    "logically or physically data quality and governance, and delivering data through an integrated approach\n",
    "for analytics purposes For example, to make customer data available\n",
    "for analytics, you would need to extract individual customers' information from operational systems\n",
    "such as sales, marketing, and finance. You would then need to provide a unified view\n",
    "of the combined data so that your users can access, query, and manipulate this data from\n",
    "a single interface to derive statistics, analytics, and visualizations. How does a data integration platform relate\n",
    "to ETL and data pipelines? While data integration combines disparate\n",
    "data into a unified view of the data, a data pipeline covers the entire data movement journey\n",
    "from source to destination systems. In that sense, you use a data pipeline to\n",
    "perform data integration, while ETL is a process within data integration. There is no one approach to data integration. However, modern data integration solutions\n",
    "typically support the following capabilities: An extensive catalog of pre-built connectors\n",
    "and adopters that help you connect and build integration flows with a wide variety of data\n",
    "sources such as databases, flat files, social media data, APIs, CRM and ERP applications. Open-source architecture that provides greater\n",
    "flexibility and avoids vendor lock-in. Optimization for both batch processing of\n",
    "large-scale data and continuous data streams, or both. Integration with Big Data sources. Support for big data is increasingly driving\n",
    "the decision regarding choice of integration platforms. Additional functionalities. For example, specific demands around data\n",
    "quality and governance, compliance, and security. Portability, which ensures that as businesses\n",
    "move to cloud models, they should be able to run their data integration platforms anywhere. And data integration tools are able to work\n",
    "natively in a single cloud, multi-cloud, or hybrid cloud environment. There are many data integration platforms\n",
    "and tools available in the market, ranging from commercial off-the-shelf tools to open-source\n",
    "frameworks. IBM offers a host of data integration tools\n",
    "targeting a range of enterprise integration scenarios, such as Information Server for\n",
    "IBM, Cloud Pak for Data, IBM Cloud Pak for Integration, IBM Data Replication, IBM Data\n",
    "Virtualization Manager, IBM InfoSphere Information Server on Cloud, and IBM InfoSphere DataStage\n",
    "all target a range of enterprise data integration scenarios. Talend's data integration tools include Talend\n",
    "Data Fabric, Talend Cloud, Talend Data Catalog, Talend Data Management, Talend Big Data, Talend\n",
    "Data Services, and Talend Open Studio. SAP, Oracle, Denodo, SAS, Microsoft, Qlik,\n",
    "and TIBCO are some of the other vendors that offer data integration tools and platforms. Examples of open-source frameworks include\n",
    "Dell Boomi, Jitterbit, and SnapLogic. There are a significant number of vendors\n",
    "who are offering cloud-based Integration Platform as a Service, or iPaaS, as a hosted service\n",
    "via virtual private cloud or hybrid cloud. Such as the Adeptia Integration Suite, Google\n",
    "Cloud's Cooperation 534, IBM's Application Integration Suite on Cloud, and Informatica's\n",
    "Integration Cloud. The data integration space continues to evolve\n",
    "as businesses embrace newer technologies and as data grows, be it in the variety of sources\n",
    "or its use in business decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907681e7-4434-4d63-b924-4bbacae94e7c",
   "metadata": {},
   "source": [
    "### NoSQL and Data Lake"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d869c86-09df-4a5b-8a8a-9c1ea5402562",
   "metadata": {},
   "source": [
    "They are complementary — they serve different purposes in the data stack, but can interact and support each other.\n",
    "A Data Lake is a centralized repository designed to store vast amounts of raw, semi-structured, and unstructured data—such as logs, images, JSON files, or Parquet tables—at low cost and in flexible formats, making it ideal for long-term storage and large-scale analytics using engines like Spark or Presto. In contrast, a NoSQL database is optimized for fast, scalable, real-time access to semi-structured data, and is commonly used in applications that require quick reads and writes, such as user sessions, product catalogs, or event tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0b9c6-b812-41bc-849e-f3fe62c7238b",
   "metadata": {},
   "source": [
    "### OLTP family and OLAP family"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8aae5eb1-1c4c-423f-84a5-53d0208e54d1",
   "metadata": {},
   "source": [
    "RDBs and Data Warehouses are the classical foundations of OLTP and OLAP.\n",
    "In the era of big data, NoSQL databases and Lakehouses—which combine Data Lakes with processing and query engines like Spark, Presto, or Dremio—emerge as their modern counterparts. These systems are capable of handling not only structured data, but also the semi-structured and unstructured formats that dominate today’s digital landscape. They serve similar goals—fast transactions (OLTP family) and deep analytics (OLAP family)—but rely on more flexible, scalable architectures designed for the demands of high-volume, high-velocity data environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a68f238-5106-4d32-bbed-80554b7f43bd",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e68c3b69-fbde-4498-9bba-cb9dc6d5f8af",
   "metadata": {},
   "source": [
    "A Data Repository is a general term that refers to data that has been collected, organized, and isolated so that it can be used for reporting, analytics, and also for archival purposes. \n",
    "\n",
    "The different types of Data Repositories include:\n",
    "\n",
    "Databases, which can be relational or non-relational, each following a set of organizational principles, the types of data they can store, and the tools that can be used to query, organize, and retrieve data.\n",
    "\n",
    "Data Warehouses, that consolidate incoming data into one comprehensive store house. \n",
    "\n",
    "Data Marts, that are essentially sub-sections of a data warehouse, built to isolate data for a particular business function or use case.\n",
    "\n",
    "Data Lakes, that serve as storage repositories for large amounts of structured, semi-structured, and unstructured data in their native format.\n",
    "\n",
    "Big Data Stores, that provide distributed computational and storage infrastructure to store, scale, and process very large data sets.\n",
    "\n",
    "The ETL, or Extract Transform and Load, Process is an automated process that converts raw data into analysis-ready data by:\n",
    "\n",
    "Extracting data from source locations.\n",
    "\n",
    "Transforming raw data by cleaning, enriching, standardizing, and validating it.\n",
    "\n",
    "Loading the processed data into a destination system or data repository.\n",
    "\n",
    "The ELT, or Extract Load and Transfer, Process is a variation of the ETL Process. In this process, extracted data is loaded into the target system before the transformations are applied. This process is ideal for Data Lakes and working with Big Data.\n",
    "\n",
    "Data Pipeline, sometimes used interchangeably with ETL and ELT, encompasses the entire journey of moving data from its source to a destination data lake or application, using the ETL or ELT process.\n",
    "\n",
    "Data Integration Platforms combine disparate sources of data, physically or logically, to provide a unified view of the data for analytics purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef578e8-f0f3-4882-b080-d0ebc6164bd3",
   "metadata": {},
   "source": [
    "## Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f33d91-7ae6-4b61-8a5f-3ad1c390ad6f",
   "metadata": {},
   "source": [
    "### Hadoop, Kafka, and Spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc515202-11ad-4fcd-a6dd-1d2f54eed03c",
   "metadata": {},
   "source": [
    "In the big data ecosystem, Hadoop, Kafka, and Spark each play distinct but complementary roles, forming a powerful triad for managing and analyzing massive amounts of data. Hadoop provides the foundational layer for storing data in a distributed and fault-tolerant manner through its HDFS (Hadoop Distributed File System), and it originally handled processing via MapReduce, though that’s now often replaced with more modern tools like Spark. Kafka, on the other hand, is a high-throughput, distributed messaging system used to stream real-time data between systems — for instance, capturing user actions, application logs, or events from various sources and publishing them in real-time pipelines. Kafka itself doesn’t process or store data long-term, but it’s excellent for reliably transporting data. This is where Apache Spark enters the picture: it acts as the computational engine that can ingest data from both Hadoop (for batch processing) and Kafka (for stream processing), then transform, analyze, and output results — all at scale and often in-memory, which makes it much faster than older MapReduce models. Spark not only handles both batch and real-time data workflows efficiently, but it also supports advanced analytics like SQL queries, machine learning, and graph computation — making it the brain that connects and processes the data Kafka transports and Hadoop stores. Together, these tools provide an end-to-end pipeline: Kafka moves data, Hadoop stores it, and Spark analyzes and transforms it — whether in real-time or in scheduled jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fdca8c-c9bd-4f26-a4b4-3b26437e9b99",
   "metadata": {},
   "source": [
    "### Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccdb3adc-a959-4a90-99be-e65d8f67908e",
   "metadata": {},
   "source": [
    "The Big Data processing technologies provide\n",
    "ways to work with large sets of structured, semi-structured, and unstructured data so\n",
    "that value can be derived from big data. In some of the other videos, we discussed\n",
    "Big Data technologies such as NoSQL databases and Data Lakes. In this video, we are going to talk about\n",
    "three open source technologies and the role they play in big data analytics — ApacheHadoop,\n",
    "Apache Hive, and Apache Spark. Hadoop is a collection of tools that provides\n",
    "distributed storage and processing of big data. Hive is a data warehouse for data query and\n",
    "analysis built on top of Hadoop. Spark is a distributed data analytics framework\n",
    "designed to perform complex data analytics in real-time. Hadoop, a java-based open-source framework,\n",
    "allows distributed storage and processing of large datasets across clusters of computers. In Hadoop distributed system, a node is a\n",
    "single computer, and a collection of nodes forms a cluster. Hadoop can scale up from a single node to\n",
    "any number of nodes, each offering local storage and computation. Hadoop provides a reliable, scalable, and\n",
    "cost-effective solution for storing data with no format requirements. Using Hadoop, you can: Incorporate emerging data formats, such as\n",
    "streaming audio, video, social media sentiment, and clickstream data, along with structured,\n",
    "semi-structured, and unstructured data not traditionally used in a data warehouse. Provide real-time, self-service access for\n",
    "all stakeholders. Optimize and streamline costs in your enterprise\n",
    "data warehouse by consolidating data across the organization and moving “cold” data,\n",
    "that is, data that is not in frequent use, to a Hadoop-based system. One of the four main components of Hadoop\n",
    "is Hadoop Distributed File System, or HDFS, which is a storage system for big data that\n",
    "runs on multiple commodity hardware connected through a network. HDFS provides scalable and reliable big data\n",
    "storage by partitioning files over multiple nodes. It splits large files across multiple computers,\n",
    "allowing parallel access to them. Computations can, therefore, run in parallel\n",
    "on each node where data is stored. It also replicates file blocks on different\n",
    "nodes to prevent data loss, making it fault-tolerant. Let’s understand this through an example. Consider a file that includes phone numbers\n",
    "for everyone in the United States; the numbers for people with last name starting with A\n",
    "might be stored on server 1, B on server 2, and so on. With Hadoop, pieces of this phonebook would\n",
    "be stored across the cluster. To reconstruct the entire phonebook, your\n",
    "program would need the blocks from every server in the cluster. HDFS also replicates these smaller pieces\n",
    "onto two additional servers by default, ensuring availability when a server fails, In addition to higher availability, this offers\n",
    "multiple benefits. It allows the Hadoop cluster to break up work\n",
    "into smaller chunks and run those jobs on all servers in the cluster for better scalability. Finally, you gain the benefit of data locality,\n",
    "which is the process of moving the computation closer to the node on which the data resides. This is critical when working with large data\n",
    "sets because it minimizes network congestion and increases throughput. Some of the other benefits that come from\n",
    "using HDFS include: Fast recovery from hardware failures, because\n",
    "HDFS is built to detect faults and automatically recover. Access to streaming data, because HDFS supports\n",
    "high data throughput rates. Accommodation of large data sets, because\n",
    "HDFS can scale to hundreds of nodes, or computers, in a single cluster. Portability, because HDFS is portable across\n",
    "multiple hardware platforms and compatible with a variety of underlying operating systems. Hive is an open-source data warehouse software\n",
    "for reading, writing, and managing large data set files that are stored directly in either\n",
    "HDFS or other data storage systems such as Apache HBase. Hadoop is intended for long sequential scans\n",
    "and, because Hive is based on Hadoop, queries have very high latency—which means Hive\n",
    "is less appropriate for applications that need very fast response times. Also, Hive is read-based, and therefore not\n",
    "suitable for transaction processing that typically involves a high percentage of write operations. Hive is better suited for data warehousing\n",
    "tasks such as ETL, reporting, and data analysis and includes tools that enable easy access\n",
    "to data via SQL. This brings us to Spark, a general-purpose\n",
    "data processing engine designed to extract and process large volumes of data for a wide\n",
    "range of applications, including Interactive Analytics, Streams Processing, Machine Learning,\n",
    "Data Integration, and ETL. It takes advantage of in-memory processing\n",
    "to significantly increase the speed of computations and spilling to disk only when memory is constrained. Spark has interfaces for major programming\n",
    "languages, including Java, Scala, Python, R, and SQL. It can run using its standalone clustering\n",
    "technology as well as on top of other infrastructures such as Hadoop. And it can access data in a large variety\n",
    "of data sources, including HDFS and Hive, making it highly versatile. The ability to process streaming data fast\n",
    "and perform complex analytics in real-time is the key use case for Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f543844-0598-4cf3-aafa-e2e014ffbd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
