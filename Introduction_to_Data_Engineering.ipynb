{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3209559f-1c78-48b9-aa16-f4b0e1f80088",
   "metadata": {},
   "source": [
    "# What is Data Engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17c5c6-740a-4b3f-8c4b-9b0d2edbb62a",
   "metadata": {},
   "source": [
    "## Modern data ecosystem and role of data engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffcf96-7188-448a-a0f2-bf8eeae1daef",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffeb04ff-1762-469f-b4db-f9fc0f4137a7",
   "metadata": {},
   "source": [
    "Modern data ecosystem includes a network of interconnected and continually evolving entities that include:\n",
    "\n",
    "Data, that is available in a host of different formats, structures, and sources. \n",
    "\n",
    "Enterprise Data Environment, in which raw data is staged so it can be organized, cleaned, and optimized for use by end-users.\n",
    "\n",
    "End-users, such as business stakeholders, analysts, and programmers who consume data for various purposes.\n",
    "\n",
    "Emerging technologies such as Cloud Computing, Machine Learning, and Big Data, are continually reshaping the data ecosystem and the possibilities it offers.\n",
    "\n",
    "Data Engineers, Data Analysts, Data Scientists, Business Analysts, and Business Intelligence Analysts, all play a vital role in the ecosystem for deriving insights and business results from data.\n",
    "\n",
    "The goal of Data Engineering is to make quality data available for analytics and decision-making. And it does this by collecting raw source data, processing data so it becomes usable, storing data, and making quality data available to users securely.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40915b74-497a-43da-b22c-9b0e573efcd8",
   "metadata": {},
   "source": [
    "## Responsibilities and Skillsets of a Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d3c27-bb53-4821-98c5-cda6af870c00",
   "metadata": {},
   "source": [
    "### Technical skills"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09e2946a-d8d9-49ad-92a2-b3f9ef516020",
   "metadata": {},
   "source": [
    "Knowledge of working with operating systems such as UNIX, Linux, and Windows, including commonly used administrative tools, system utilities and commands.\n",
    "Knowledge of infrastructure components, such as virtual machines, networking, and application services, such as load balancing and application performance monitoring.\n",
    "Also, cloud-based services such as those offered by Amazon, Google, IBM, and Microsoft.\n",
    "Experience of working with databases and data warehouses, which include: RDBMSes such as IBM DB2, MySQL, Oracle Database, and PostgreSQL. NoSQL databases such as Redis, MongoDB, Cassandra, and Neo4J. Data warehouses such as Oracle Exadata, IBM Db2 Warehouse on Cloud, IBM Netezza Performance Server, and Amazon RedShift.\n",
    "A high-level of proficiency working with data pipelines. Popular data pipeline solutions include Apache Beam, AirFlow, and DataFLow.\n",
    "Experience of working with ETL tools such as IBM Infosphere Information Server, AWS Glue, and Improvado.\n",
    "Proficiency in languages for querying, manipulating, and processing data. This includes: Query languages for accessing and manipulating data in a database, such as SQL for relational databases and SQL-like query languages for NoSQL databases.\n",
    "Programming languages such as Python, R, and Java. Shell and Scripting languages, such as Unix/Linux Shell and PowerShell.\n",
    "Familiarity with Big Data processing tools such as Hadoop, Hive, and Spark.\n",
    "##\n",
    "Kafka is part of the data pipeline ecosystem, but it's not an ETL tool or orchestrator. It's best described as a real-time event streaming platform that enables pipelines, feeds ETL tools, and supports orchestration.\n",
    "##\n",
    "Kafka, Spark, Storm are applications for processing data streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6adf4c-55a0-4d3e-99e9-a5accb6db592",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ebcc8c3-14ce-4cd5-a44f-2327b24f0c04",
   "metadata": {},
   "source": [
    "The role of a Data Engineer includes:\n",
    "\n",
    "Gathering data from disparate sources.\n",
    "\n",
    "Integrating data into a unified view for data consumers.\n",
    "\n",
    "Preparing data for analytics and reporting.\n",
    "\n",
    "Managing data pipelines for a continuous flow of data from source to destination systems.\n",
    "\n",
    "Managing the complete infrastructure for the collection, processing, and storage of data.\n",
    "\n",
    "To be successful in their role, Data Engineers need a mix of technical, functional, and soft skills.\n",
    "\n",
    "Technical Skills include working with different operating systems and infrastructure components such as virtual machines, networks, and application services. It also includes working with databases and data warehouses, data pipelines, ETL tools, big data processing tools, and languages for querying, manipulating, and processing data. \n",
    "\n",
    "An understanding of the potential application of data in business is an important skill for a data engineer. Other functional skills include the ability to convert business requirements into technical specifications, an understanding of the software development lifecycle, and the areas of data quality, privacy, security, and governance. \n",
    "\n",
    "Soft Skills include interpersonal skills, the ability to work collaboratively, teamwork, and effective communication. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbacbd7-3b7c-4427-9cbe-e550b2e9c0f1",
   "metadata": {},
   "source": [
    "## The Data Ecosystem and Languages for Data Professionals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab35183-9375-424b-8104-ffe409516c44",
   "metadata": {},
   "source": [
    "### Metadata and Metadata Management"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb94692d-e97d-4fc8-a485-0711d38857d7",
   "metadata": {},
   "source": [
    "Objectives\n",
    "After completing this reading, you will be able to:\n",
    "\n",
    "Define what metadata is\n",
    "Describe what metadata management is\n",
    "Explain the importance of metadata management\n",
    "List popular tools for metadata management\n",
    "What is metadata?\n",
    "Metadata is data that provides information about other data.\n",
    "\n",
    "This is a very broad definition. Here we will consider the concept of metadata within the context of databases, data warehousing, business intelligence systems, and all kinds of data repositories and platforms.\n",
    "\n",
    "We'll consider the following three main types of metadata:\n",
    "\n",
    "Technical metadata\n",
    "Process metadata, and\n",
    "Business metadata\n",
    "Technical metadata\n",
    "Technical metadata is metadata which defines the data structures in data repositories or platforms, primarily from a technical perspective.\n",
    "\n",
    "For example, technical metadata in a data warehouse includes assets such as:\n",
    "\n",
    "Tables that record information about the tables stored in a database, like:\n",
    "\n",
    "each table's name\n",
    "the number of columns and rows each table has\n",
    "A data catalog, which is an inventory of tables that contain information, like:\n",
    "\n",
    "the name of each database in the enterprise data warehouse\n",
    "the name of each column present in each database\n",
    "the names of every table that each column is contained in\n",
    "the type of data that each column contains\n",
    "The technical metadata for relational databases is typically stored in specialized tables in the database called the System Catalog.\n",
    "\n",
    "Process metadata\n",
    "Process metadata describes the processes that operate behind business systems such as data warehouses, accounting systems, or customer relationship management tools.\n",
    "\n",
    "Many important enterprise systems are responsible for collecting and processing data from various sources. Such critical systems need to be monitored for failures and any performance anomalies that arise. Process metadata for such systems includes tracking things like:\n",
    "\n",
    "process start and end times\n",
    "disk usage\n",
    "where data was moved from and to, and\n",
    "how many users access the system at any given time\n",
    "This sort of data is invaluable for troubleshooting and optimizing workflows and ad hoc queries.\n",
    "\n",
    "Business metadata\n",
    "Users who want to explore and analyze data within and outside the enterprise are typically interested in data discovery. They need to be able to find data which is meaningful and valuable to them and know where that data can be accessed from. These business-minded users are thus interested in business metadata, which is information about the data described in readily interpretable ways, such as:\n",
    "\n",
    "how the data is acquired\n",
    "what the data is measuring or describing\n",
    "the connection between the data and other data sources\n",
    "Business metadata also serves as documentation for the entire data warehouse system.\n",
    "\n",
    "Managing metadata\n",
    "Managing metadata includes developing and administering policies and processes to ensure information can be accessed and integrated from various sources and appropriately shared across the entire enterprise.\n",
    "\n",
    "Creation of a reliable, user-friendly data catalog is a primary objective of a metadata management model. The data catalog is a core component of a modern metadata management system, serving as the main asset around which metadata management is administered. It serves as the basis by which companies can inventory and efficiently organize their data systems. A modern metadata management model will include a web-based user interface that enables engineers and business users to easily search for and find information on key attributes such as CustomerName or ProductType. This kind of model is central to any Data Governance initiative.\n",
    "\n",
    "Why is metadata management important?\n",
    "Good metadata management has many valuable benefits. Having access to a well implemented data catalog greatly enhances data discovery, repeatability, governance, and can also facilitate access to data.\n",
    "\n",
    "Well managed metadata helps you to understand both the business context associated with the enterprise data and the data lineage, which helps to improve data governance. Data lineage provides information about the origin of the data and how it gets transformed and moved, and thus it facilitates tracing of data errors back to their root cause. Data governance is a data management concept concerning the capability that enables an organization to ensure that high data quality exists throughout the complete lifecycle of the data, and data controls are implemented that support business objectives.\n",
    "\n",
    "The key focus areas of data governance include availability, usability, consistency, data integrity and data security and includes establishing processes to ensure effective data management throughout the enterprise such as accountability for the adverse effects of poor data quality and ensuring that the data which an enterprise has can be used by the entire organization.\n",
    "\n",
    "Popular tools for metadata management\n",
    "Popular metadata management tools include:\n",
    "\n",
    "IBM InfoSphere Information Server\n",
    "CA Erwin Data Modeler\n",
    "Oracle Warehouse Builder\n",
    "SAS Data Integration Server\n",
    "Talend Data Fabric\n",
    "Alation Data Catalog\n",
    "SAP Information Steward\n",
    "Microsoft Azure Data Catalog\n",
    "IBM Watson Knowledge Catalog\n",
    "Oracle Enterprise Metadata Management (OEMM)\n",
    "Adaptive Metadata Manager\n",
    "Unifi Data Catalog\n",
    "data.world\n",
    "Informatica Enterprise Data Catalog\n",
    "Summary\n",
    "In this reading, you learned that:\n",
    "\n",
    "Metadata is data that provides information about other data, and includes three main types: technical, process, and business metadata\n",
    "The technical metadata for relational databases is typically stored in specialized tables in the database called the system catalog\n",
    "A primary objective of business metadata management modelling is the creation and maintenance of a reliable, user-friendly data catalog\n",
    "Having access to a well-implemented data catalog greatly enhances data discovery, repeatability, governance, and can also facilitate access to data\n",
    "Metadata management tools from IBM include InfoSphere Information Server and Watson Knowledge Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdcb0f-0314-4970-a716-cbe1ce510f5c",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9f40ed1-e0a1-4f73-b3c1-3542db695bc5",
   "metadata": {},
   "source": [
    "Summary and Highlights\n",
    "In this lesson, you have learned:\n",
    "\n",
    "A Data Engineer’s ecosystem includes the infrastructure, tools, frameworks, and processes for extracting data, architecting and managing data pipelines and data repositories, managing workflows, developing applications, and managing BI and Reporting tools.\n",
    "\n",
    "Based on how well-defined the structure of the data is, data can be categorized as\n",
    "\n",
    "Structured data, that is data which is well organized in formats that can be stored in databases.\n",
    "\n",
    "Semi-structured data, that is data which is partially organized and partially free-form.\n",
    "\n",
    "Unstructured data, that is data which can not be organized conventionally into rows and columns.\n",
    "\n",
    "Data comes in a wide-ranging variety of file formats, such as, delimited text files, spreadsheets, XML, PDF, and JSON, each with its own list of benefits and limitations of use.\n",
    "\n",
    "Data is extracted from multiple data sources, ranging from relational and non-relational databases, to APIs, web services, data streams, social platforms, and sensor devices.\n",
    "\n",
    "Once the data is identified and gathered from different sources, it needs to be staged in a data repository so that it can be prepared for analysis. The type, format, and sources of data influence the type of data repository that can be used.\n",
    "\n",
    "Data professionals need a host of languages that can help them extract, prepare, and analyse data. These can be classified as:\n",
    "\n",
    "Querying languages, such as SQL, used for accessing and manipulating data from databases.\n",
    "\n",
    "Programming languages such as Python, R, and Java, for developing applications and controlling application behavior.\n",
    "\n",
    "Shell and Scripting languages, such as Unix/Linux Shell, and PowerShell, for automating repetitive operational tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b032e10-79ed-4cb6-ad60-409b05f0fce5",
   "metadata": {},
   "source": [
    "## Data Repositories, Data Pipelines, and Data Integration Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a9cf2-c3b8-4e2e-8c35-017f587113d9",
   "metadata": {},
   "source": [
    "### NoSQL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce029bec-06e4-421a-bc2b-4571d0175c3a",
   "metadata": {},
   "source": [
    "NoSQL, which stands for “not only SQL,”\n",
    "or sometimes “non SQL” is a non-relational database design that provides flexible schemas\n",
    "for the storage and retrieval of data. NoSQL databases have existed for many years\n",
    "but have only recently become more popular in the era of cloud, big data, and high-volume\n",
    "web and mobile applications. They are chosen today for their attributes\n",
    "around scale, performance,and ease of use. It's important to emphasize that the \"No\"\n",
    "in \"NoSQL\" is an abbreviation for \"not only\" and not the actual word \"No.\" NoSQL databases are built for specific data\n",
    "models and have flexible schemas that allow programmers to create and manage modern applications. They do not use a traditional row/column/table\n",
    "database design with fixed schemas, and typically not use the structured query language (or\n",
    "SQL) to query data, although some may support SQL or SQL-like interfaces. NoSQLallows data to be stored in a schema-less\n",
    "or free-form fashion. Any data, be It structured, semi-structured,\n",
    "or unstructured,can be stored in any record. Based on the model being used for storing\n",
    "data, there are four common types of NoSQL databases: Key-value store, Document-based, Column-based, and graph-based. Key-value store: Data in a key-value database\n",
    "is stored as a collection of key-value pairs. The key represents an attribute of the data\n",
    "and is a unique identifier. Both keys and values can be anything from\n",
    "simple integers or strings to complex JSON documents. Key-value stores are great for storing user\n",
    "session dataanduser preferences, making real-time recommendations and targeted advertising,\n",
    "and in-memory data caching. However, if you want to be able to query the\n",
    "data on specific data value, need relationships between data values, or need to have multiple\n",
    "unique keys, a key-value store may not be the best fit. Redis, Memcached, and DynamoDB are some well-known\n",
    "examples in this category. Document-based: Document databasesstore each\n",
    "record and its associated data within a single document. They enable flexible indexing, powerful ad\n",
    "hoc queries, and analytics over collections of documents. Document databases are preferable for eCommerce\n",
    "platforms, medical records storage, CRM platforms, and analytics platforms. However, if you’re looking to run complex\n",
    "search queries and multi-operation transactions, a document-based database may not be the best\n",
    "option for you. MongoDB, DocumentDB, CouchDB, and Cloudant\n",
    "are some of the popular document-based databases. Column-based: Column-based models store data\n",
    "in cells grouped as columns of data instead of rows. A logical grouping of columns, that is, columns\n",
    "that are usually accessed together, is called a column family. For example, a customer’s name and profile\n",
    "information will most likely be accessed together but not their purchase history. So,customer name and profile information data\n",
    "can be grouped into a column family. Since column databases store all cells corresponding\n",
    "to a column as a continuous disk entry, accessing and searching the data becomes very fast. Column databases can be great for systems\n",
    "that require heavy write requests, storing time-series data, weather data, and IoT data. But if you need to use complex queries or\n",
    "change your querying patterns frequently, this may not be the best option for you. The most popular column databases are Cassandra\n",
    "and HBase. Graph-based: Graph-based databases use a graphical\n",
    "model to represent and store data. They are particularly useful for visualizing,\n",
    "analyzing, and finding connections between different pieces of data. The circles arenodes, and they contain the\n",
    "data. The arrows represent relationships. Graph databases are an excellent choice for\n",
    "working with connected data, which is data that contains lots of interconnected relationships. Graph databases are great for social networks,\n",
    "real-time product recommendations, network diagrams, fraud detection, and access management. But if you want to process high volumes of\n",
    "transactions, it may not be the best choice for you, because graph databases are not optimized\n",
    "for large-volume analytics queries. Neo4J and CosmosDB are some of the more popular\n",
    "graph databases. Advantages of NoSQLNoSQL was created in response\n",
    "to the limitations of traditional relational database technology. The primary advantage of NoSQL is its ability\n",
    "to handle large volumes of structured, semi-structured, and unstructured data. Some of its other advantages include: The ability to run as distributed systemsscaled\n",
    "across multiple data centers, which enables them to take advantage of cloud computing\n",
    "infrastructure; An efficient and cost-effective scale-out\n",
    "architecture that provides additional capacity and performance with the addition of new nodes;\n",
    "and Simpler design, better control over availability,\n",
    "and improved scalability that enables you to be more agile, more flexible, and to iterate\n",
    "more quickly To summarizethe key differencesbetween relational\n",
    "and non-relational databases: RDBMS schemas rigidly define how all data\n",
    "inserted into the database must be typed and composed, whereas NoSQL databases can be schema-agnostic,\n",
    "allowing unstructured and semi-structured data to be stored and manipulated. Maintaining high-end, commercial relational\n",
    "database management systems is expensive whereas NoSQL databases are specifically designed\n",
    "for low-cost commodity hardware Relational databases, unlike most NoSQL, support\n",
    "ACID-compliance, which ensures reliability of transactions and crash recovery. RDBMS is a mature and well-documented technology,\n",
    "which means the risks are more or less perceivable as compared to NoSQL, which is a relatively\n",
    "newer technology. Nonetheless, NoSQL databases are here to stay,\n",
    "and are increasingly being used for mission critical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54e785-11b7-4bad-b4e3-e5a2bfaf1d48",
   "metadata": {},
   "source": [
    "### Data Warehouses, Data Marts, and Data Lakes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f2afd7c-4a93-480a-a9b8-1f6c87add020",
   "metadata": {},
   "source": [
    "All data mining repositories have a similar\n",
    "goal, which is to house data for reporting, analysis, and deriving insights. But their purpose, types of data stored, and\n",
    "how data is accessed differs. In this video, we will learn about some of\n",
    "the characteristics and applications of data warehouses, data marts, and data lakes. A data warehouse is a central repository of\n",
    "data integrated from multiple sources. Data warehouses serve as the single source\n",
    "of truth—storing current and historical data that has been cleansed, conformed, and\n",
    "categorized. When data gets loaded into the data warehouse,\n",
    "it is already modeled and structured for a specific purpose, meaning it's analysis-ready. Traditionally, data warehouses are known to\n",
    "store relational data from transactional systems and operational databases such as CRM, ERP,\n",
    "HR, and Finance applications. But with the emergence of NoSQL technologies\n",
    "and new data sources, non-relational data repositories are also being used for data\n",
    "warehousing. Typically, a data warehouse has a three-tier\n",
    "architecture: The bottom tier of the architecture includes\n",
    "the database servers, which could be relational, non-relational, or both, that extract data\n",
    "from different sources. The middle tier of the architecture consists\n",
    "of the OLAP Server, a category of software that allows users to process and analyze information\n",
    "coming from multiple database servers. And the topmost tier of the architecture includes\n",
    "the client front-end layer. This tier includes all the tools and applications\n",
    "used for querying, reporting, and analyzing data. In response to the rapid data growth and today's\n",
    "sophisticated analytics tools, data warehouses that once resided in on-premise data centers\n",
    "are moving to the cloud. Compared to their on-premise versions, some\n",
    "of the benefits offered by cloud-based data warehouses include: Lower costs, Limitless storage and compute capabilities, Scale on a pay-as-you-go basis; and Faster disaster recovery. As an organization, you would opt for a data\n",
    "warehouse when you have massive amounts of data from your operational systems that need\n",
    "to be readily available for reporting and analysis. Some of the popularly used data warehouses\n",
    "include Teradata Enterprise Data Warehouse platform, Oracle Exadata, IBM Db2 Warehouse\n",
    "on Cloud, IBM Netezza Performance Server, Amazon RedShift, BigQuery by Google Cloudera's\n",
    "Enterprise Data Hub, and Snowflake Cloud Data Warehouse. A data mart is a sub-section of the data warehouse,\n",
    "built specifically for a particular business function, purpose, or community of users. For example, sales or finance groups in an\n",
    "organization accessing data for their quarterly reporting and projections. There are three basic types of data marts—dependent,\n",
    "independent, and hybrid data marts. Dependent data marts are a sub-section of\n",
    "an enterprise data warehouse. Since a dependent data mart offers analytical\n",
    "capabilities for a restricted area of the data warehouse, it also provides isolated\n",
    "security and isolated performance. Independent data marts are created from sources\n",
    "other than an enterprise data warehouse, such as internal operational systems or external\n",
    "data. Hybrid data marts combine inputs from data\n",
    "warehouses, operational systems, and external systems. The difference also lies in how data is extracted\n",
    "from the source systems, the transformations that need to be applied, and how the data\n",
    "is transported into the mart. Dependent data marts, for example, pull data\n",
    "from an enterprise data warehouse, where data has already been cleaned and transformed. Independent data marts need to carry out the\n",
    "transformation process on the source data since it is coming directly from operational\n",
    "systems and external sources. Whatever the type, the purpose of a data mart\n",
    "is to: provide users' data that is most relevant\n",
    "to them when they need it, accelerate business processes by providing\n",
    "efficient response times, provide a cost and time-efficient way in which\n",
    "data-driven decisions can be taken, improve end-user response time; and provide secure access and control. A Data Lake is a data repository that can\n",
    "store large amounts of structured, semi-structured, and unstructured data in their native format. While a data warehouse stores data that has\n",
    "been cleaned, processed, and transformed for a specific need, you do not need to define\n",
    "the structure and schema of data before loading into the data lake. You do not even need to know all of the use\n",
    "cases for which you will ultimately be analyzing the data. A data lake exists as a repository of raw\n",
    "data in its native format, straight from the source, to be transformed based on the use\n",
    "case for which it needs to be analyzed. Which does not mean that a data lake is a\n",
    "place where data can be dumped without governance. While in the data lake, the data is appropriately\n",
    "classified, protected, and governed. A data lake is a reference architecture that\n",
    "is independent of technology. Data lakes combine a variety of technologies\n",
    "that come together to facilitate agile data exploration for analysts and data scientists. Data lakes can be deployed using Cloud Object\n",
    "Storage, such as Amazon S3, or large-scale distributed systems such as Apache Hadoop,\n",
    "used for processing Big Data. They can also be deployed on different relational\n",
    "database management systems, as well as NoSQL data repositories that can store very large\n",
    "amounts of data. Data lakes offer a number of benefits, such\n",
    "as: The ability to store all types of data – unstructured\n",
    "data such as documents, emails, PDFs, semi-structured data such as JSON, XML, CSV, and logs, as\n",
    "well as structured data from relational databases The agility to scale based on storage capacity\n",
    "– growing from terabytes to petabytes of data Saving time in defining structures, schemas,\n",
    "and transformations since data is imported in its original format and The ability to repurpose data in several different\n",
    "ways and wide-ranging use cases. This is extremely beneficial as it is hard\n",
    "for businesses to foresee all the different ways in which you could potentially leverage\n",
    "their data in the future. Some of the vendors that provide technologies,\n",
    "platforms, and reference architectures for data lakes include Amazon, Cloudera, Google,\n",
    "IBM, Informatica, Microsoft, Oracle, SAS, Snowflake, Teradata, and Zaloni. In this video, we learned about some of the\n",
    "capabilities of data mining repositories such as data warehouses, data marts, and data lakes. While they all have a similar goal, they need\n",
    "to be evaluated within the context of the use case and technology infrastructure for\n",
    "selecting the one that works best for an organization’s needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d49c7-714f-4a77-9669-6de8f6b39348",
   "metadata": {},
   "source": [
    "### Data Lakehouses Explained"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0cb8554-a756-4834-905c-2f240f56ed48",
   "metadata": {},
   "source": [
    "So last week, I'm having dinner at this\n",
    "restaurant and I'm looking around, the place is packed,\n",
    "everyone's getting their orders on time. And I couldn't help but\n",
    "think about the logistics that go into a restaurant turning raw\n",
    "ingredients into these delicious meals. So let's think about this for a minute. So, in a commercial kitchen, we have raw ingredients being delivered by trucks to our loading dock on large pallets, right? So truck comes in to the loading dock,\n",
    "they drop off the pallet, and the truck is back out on the road to deliver more\n",
    "ingredients to other restaurants. So that's the easy part. Now we actually have to unwrap\n",
    "this pallet and process it, right? We have to sort everything on it. We have to label all of our ingredients,\n",
    "right? And then we also have to make sure that\n",
    "each item is routed to the correct storage area. So these things could be going\n",
    "into a pantry for dry goods, or it could also be going into\n",
    "large walk in fridges and freezers for\n",
    "things like fresh vegetables and meats. And we also have to organize\n",
    "those storage areas, right? So we've got to make sure that ingredients\n",
    "that are expiring first are used first. We've got to make sure certain ingredients\n",
    "are separated from one another for contamination reasons. And we also have to make sure that\n",
    "certain ingredients hit a very certain temperature also for food safety. And by the way, we need to do all of\n",
    "this as quickly as possible, right? To minimize things like food waste, to minimize spoilage that we\n",
    "could see from the ingredients just sitting on the truck or\n",
    "on a pallet, right? And without this process,\n",
    "the cooks in the kitchen can't really do their job\n",
    "as effectively or safely. They'd be spending a lot of their\n",
    "time just looking for ingredients and less time actually cooking and serving\n",
    "out meals to their customers, right? Okay, so\n",
    "what does this have to do with data? Well, if we think about it,\n",
    "this very same process also exists within data architectures\n",
    "of organizations. So you've got all sorts of different\n",
    "data coming into your organizations from different sources,\n",
    "such as in different cloud environments, different operational applications. Now we even have social media data, right? All this is coming into our\n",
    "organization just like a kitchen has ingredients coming from\n",
    "different suppliers, okay? So constantly have data coming in. We need a quick place to dump all\n",
    "different types of data in different formats for later use. So we have data lakes. Now, these lakes allow us to cheaply and quickly capture raw, structured, and unstructured and\n",
    "even semi structured data. Okay, so now, just like in the kitchen, we're not really cooking\n",
    "on the loading dock, right? Now, maybe I can put a tiny grill\n",
    "there if I really wanted to. But we have to organize and transform this data from its raw state\n",
    "into something that's usable for the kind of insights and analytics\n",
    "that our business wants to generate. So we have enterprise data warehouses or\n",
    "EDWs, right, where data is loaded in,\n",
    "sometimes from a data lake, but sometimes from other sources\n",
    "like operational applications. And it's optimized and organized to run very specific analytical tasks. Now, this could be powering\n",
    "different business intelligence or BI workloads such as building\n",
    "dashboards and reports, or it could be feeding into\n",
    "other analytical tools. Just like our pantries and\n",
    "freezers data in the warehouse is cleaned, organized, governed, and\n",
    "should be trusted for integrity. Okay, so what are some of the challenges\n",
    "that we see in this approach? Well, as we said, data lakes are really awesome to capture\n",
    "tons of data in a cost effective way. But we run into challenges with data governance and data quality, right? And a lot of times these data\n",
    "lakes can become data swamps. And this happens when there's a lot of\n",
    "duplicate, inaccurate, or incomplete data, making it difficult to track and\n",
    "manage assets. So if you think about it,\n",
    "what happens when that data becomes stale? Well, it loses its value in creating\n",
    "insights, the same way that ingredients go bad over time in our restaurant\n",
    "if we don't use them. So data lakes also have challenges\n",
    "with query performance. Since they're not built and optimized to\n",
    "handle the complex analytical queries, it can sometimes be tough to get\n",
    "insights out of lakes directly. Okay, so let's take a look\n",
    "at the data warehouse now. Now, these are really great\n",
    "at query performance. They're exceptional, but\n",
    "they can come at a high cost, right? Just like those big freezers\n",
    "can be very costly to run, we can't put everything\n",
    "into a data warehouse. Now, they can be better optimized\n",
    "to maintain data governance and quality, but they have limited support for semi structured and\n",
    "unstructured data sources. By the way, the ones that are growing the most\n",
    "that are coming into our organization. And they can also sometimes be\n",
    "too slow for certain types of applications that require the freshest\n",
    "data because it takes time to sort, clean, and load data into the warehouse. Okay, so what do we do here? Well, developers took a step back and\n",
    "said, hey, let's take the best of both data lakes and\n",
    "data warehouses and combine them into a new technology\n",
    "called the data lake house. So we get the flexibility and we get the cost effectiveness\n",
    "of a data lake, and we get the performance and structure of a data warehouse. So we'll talk more specifically about\n",
    "the architecture of a data lake house in a future video. But from a value point of view,\n",
    "the lake house lets us store data from the exploding number of new\n",
    "sources in a low cost way, and then leverages built\n",
    "in data management and governance layers to allow us to\n",
    "power both business intelligence and high performance machine\n",
    "learning workloads quickly. Okay, so there are plenty of ways\n",
    "that we can start using a lakehouse. We can modernize our existing data lakes,\n",
    "we can complement our data warehouses to support some of these new types of AI and\n",
    "machine learning driven workloads, but we'll also talk about\n",
    "that in the future video. So the next time you're at a restaurant, I hope you think about how the meal\n",
    "on your plate got there and the steps the ingredients took to go from\n",
    "the kitchen to the meal on your plate. Thank you. If you like this video and want to see\n",
    "more like it, please like and subscribe. If you have questions,\n",
    "please drop them in the comments below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0817a-9fa4-4937-99d2-26740c5024cf",
   "metadata": {},
   "source": [
    "### ETL, ELT, and Data Pipelines"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edec1880-5a4d-4a1c-a04a-23d416ace585",
   "metadata": {},
   "source": [
    "In this video, we will learn about some of\n",
    "the different tools and processes that work to move data from source to destination systems,\n",
    "such as: ETL, or the Extract, Transform, and Load Process ELT, or the Extract, Load, and Transform Process;\n",
    "and Data Pipelines Now we come to the process that is at the\n",
    "heart of gaining value from data. The Extract, Transform, and Load process,\n",
    "or ETL. ETL is how raw data is converted into analysis-ready\n",
    "data. It is an automated process in which you gather\n",
    "raw data from identified sources, extract the information that aligns with your\n",
    "reporting and analysis needs, clean, standardize, and transform that data\n",
    "into a format that is usable in the context of your organization; and load it into a data repository While ETL is a generic process, the actual\n",
    "job can be very different in usage, utility, and complexity. Extract is the step where data from source\n",
    "locations is collected for transformation. Data extraction could be through Batch processing, meaning source data is moved\n",
    "in large chunks from the source to the target system at scheduled intervals. Tools for batch processing include Stitch\n",
    "and Blendo. Stream processing, which means source data\n",
    "is pulled in real-time from the source and transformed while it is in transit and before\n",
    "it is loaded into the data repository. Tools for stream processing include Apache\n",
    "Samza, Apache Storm, and Apache Kafka. Transform involves the execution of rules\n",
    "and functions that convert raw data into data that can be used for analysis. For example, making date formats and units of measurement\n",
    "consistent across all source data removing duplicate data filtering out data that you do not need enriching data, for example, splitting full\n",
    "name to first, middle, and last names establishing key relationships across tables applying business rules and data validations Load is the step where processed data is transported\n",
    "to a destination system or data repository. It could be: Initial loading, that is, populating all the\n",
    "data in the repository; Incremental loading, that is, applying ongoing\n",
    "updates and modifications as needed periodically; or Full refresh, that is, erasing contents of\n",
    "one or more tables and reloading with fresh data Load verification—which includes data checks\n",
    "for missing or null values, server performance, and monitoring load failures, are important\n",
    "parts of this process step. It is vital to keep an eye on load failures\n",
    "and ensure the right recovery mechanisms are in place. ETL has historically been used for batch workloads\n",
    "on a large scale. However, with the emergence of streaming ETL\n",
    "tools, they are increasingly being used for real-time streaming event data as well. Some of the popular ETL tools available include\n",
    "IBM Infosphere Information Server, AWS Glue, Improvado, Skyvia, HEVO, and Informatica PowerCenter. Now let’s look at a variation of the ETL\n",
    "process—the Extract, Load, and Transfer, or ELT process. In the ELT process, extracted data is first\n",
    "loaded into the target system, and transformations are applied in the target system. The destination system for an ELT pipeline\n",
    "is most likely a data lake, though it can also be a data warehouse. ELT is a relatively new technology powered\n",
    "by cloud technologies. Why do we need an ELT process? ELT is useful for processing large sets of\n",
    "unstructured and non-relational data. It is ideal for data lakes where transformations\n",
    "on the data are applied once the raw data is loaded into the data lake. The ELT process has several advantages. Since the raw data is delivered directly to\n",
    "the destination system rather than a staging environment, this shortens the cycle between\n",
    "extraction and delivery. ELT paired with a data lake allows you to\n",
    "ingest volumes of raw data as immediately as the data becomes available. Compared to the ETL process, ELT affords greater\n",
    "flexibility to analysts and data scientists for exploratory data analytics. ELT transforms only that data which is required\n",
    "for a particular analysis so it can be leveraged for multiple use cases. In the ETL process, the entire structure of\n",
    "the data in the warehouse may need to be modified if it is not suited for a new use case. ELT is more suited to work with Big Data. It’s common to see the terms ETL or ELT\n",
    "and data pipelines used interchangeably. And although both move data from source to\n",
    "destination, data pipeline is a broader term that encompasses the entire journey of moving\n",
    "data from one system to another, of which ETL and ELT may be subsets. Data pipelines can be architected for batch\n",
    "processing, for streaming data, and a combination of batch and streaming data. In the case of streaming data, data processing\n",
    "or transformation, happens in a continuous flow. This is particularly useful for data that\n",
    "needs constant updating, such as data from a sensor monitoring traffic. A data pipeline is a high performing system\n",
    "that supports both long-running batch queries and smaller interactive queries. The destination for a data pipeline is typically\n",
    "a data lake, although data may also be loaded to different target destinations, such as\n",
    "another application or a visualization tool. There are a number of data pipeline solutions\n",
    "available, most popular among them being Apache Beam, AirFlow, and DataFlow. In this video, we learned about some of the\n",
    "different data movement approaches—the ETL (or Extract, Transfer, and Load process) and\n",
    "ELT (or the Extract, Load, and Transform process). We also learned about Data Pipelines, encompassing\n",
    "the complete journey of data from one system to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e37e8-97cb-4c64-8155-6d545b95c85d",
   "metadata": {},
   "source": [
    "### Data Integration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff73a12b-b282-40c2-bdf7-0d19a8d5fd5f",
   "metadata": {},
   "source": [
    "Gartner defines data integration as a discipline\n",
    "comprising the practices, architectural techniques, and tools that allow organizations to ingest,\n",
    "transform, combine, and provision data across various data types. The report further explains that data integration\n",
    "has several usage scenarios, such as data consistency across applications, master data\n",
    "management, data sharing between enterprises, and data migration and consolidation. In the field of analytics and data science,\n",
    "data integration includes accessing, queueing, or extracting data from\n",
    "operational systems transforming and merging extracted data either\n",
    "logically or physically data quality and governance, and delivering data through an integrated approach\n",
    "for analytics purposes For example, to make customer data available\n",
    "for analytics, you would need to extract individual customers' information from operational systems\n",
    "such as sales, marketing, and finance. You would then need to provide a unified view\n",
    "of the combined data so that your users can access, query, and manipulate this data from\n",
    "a single interface to derive statistics, analytics, and visualizations. How does a data integration platform relate\n",
    "to ETL and data pipelines? While data integration combines disparate\n",
    "data into a unified view of the data, a data pipeline covers the entire data movement journey\n",
    "from source to destination systems. In that sense, you use a data pipeline to\n",
    "perform data integration, while ETL is a process within data integration. There is no one approach to data integration. However, modern data integration solutions\n",
    "typically support the following capabilities: An extensive catalog of pre-built connectors\n",
    "and adopters that help you connect and build integration flows with a wide variety of data\n",
    "sources such as databases, flat files, social media data, APIs, CRM and ERP applications. Open-source architecture that provides greater\n",
    "flexibility and avoids vendor lock-in. Optimization for both batch processing of\n",
    "large-scale data and continuous data streams, or both. Integration with Big Data sources. Support for big data is increasingly driving\n",
    "the decision regarding choice of integration platforms. Additional functionalities. For example, specific demands around data\n",
    "quality and governance, compliance, and security. Portability, which ensures that as businesses\n",
    "move to cloud models, they should be able to run their data integration platforms anywhere. And data integration tools are able to work\n",
    "natively in a single cloud, multi-cloud, or hybrid cloud environment. There are many data integration platforms\n",
    "and tools available in the market, ranging from commercial off-the-shelf tools to open-source\n",
    "frameworks. IBM offers a host of data integration tools\n",
    "targeting a range of enterprise integration scenarios, such as Information Server for\n",
    "IBM, Cloud Pak for Data, IBM Cloud Pak for Integration, IBM Data Replication, IBM Data\n",
    "Virtualization Manager, IBM InfoSphere Information Server on Cloud, and IBM InfoSphere DataStage\n",
    "all target a range of enterprise data integration scenarios. Talend's data integration tools include Talend\n",
    "Data Fabric, Talend Cloud, Talend Data Catalog, Talend Data Management, Talend Big Data, Talend\n",
    "Data Services, and Talend Open Studio. SAP, Oracle, Denodo, SAS, Microsoft, Qlik,\n",
    "and TIBCO are some of the other vendors that offer data integration tools and platforms. Examples of open-source frameworks include\n",
    "Dell Boomi, Jitterbit, and SnapLogic. There are a significant number of vendors\n",
    "who are offering cloud-based Integration Platform as a Service, or iPaaS, as a hosted service\n",
    "via virtual private cloud or hybrid cloud. Such as the Adeptia Integration Suite, Google\n",
    "Cloud's Cooperation 534, IBM's Application Integration Suite on Cloud, and Informatica's\n",
    "Integration Cloud. The data integration space continues to evolve\n",
    "as businesses embrace newer technologies and as data grows, be it in the variety of sources\n",
    "or its use in business decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907681e7-4434-4d63-b924-4bbacae94e7c",
   "metadata": {},
   "source": [
    "### NoSQL and Data Lake"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d869c86-09df-4a5b-8a8a-9c1ea5402562",
   "metadata": {},
   "source": [
    "They are complementary — they serve different purposes in the data stack, but can interact and support each other.\n",
    "A Data Lake is a centralized repository designed to store vast amounts of raw, semi-structured, and unstructured data—such as logs, images, JSON files, or Parquet tables—at low cost and in flexible formats, making it ideal for long-term storage and large-scale analytics using engines like Spark or Presto. In contrast, a NoSQL database is optimized for fast, scalable, real-time access to semi-structured data, and is commonly used in applications that require quick reads and writes, such as user sessions, product catalogs, or event tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f0b9c6-b812-41bc-849e-f3fe62c7238b",
   "metadata": {},
   "source": [
    "### OLTP family and OLAP family"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8aae5eb1-1c4c-423f-84a5-53d0208e54d1",
   "metadata": {},
   "source": [
    "RDBs and Data Warehouses are the classical foundations of OLTP and OLAP.\n",
    "In the era of big data, NoSQL databases and Lakehouses—which combine Data Lakes with processing and query engines like Spark, Presto, or Dremio—emerge as their modern counterparts. These systems are capable of handling not only structured data, but also the semi-structured and unstructured formats that dominate today’s digital landscape. They serve similar goals—fast transactions (OLTP family) and deep analytics (OLAP family)—but rely on more flexible, scalable architectures designed for the demands of high-volume, high-velocity data environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a68f238-5106-4d32-bbed-80554b7f43bd",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e68c3b69-fbde-4498-9bba-cb9dc6d5f8af",
   "metadata": {},
   "source": [
    "A Data Repository is a general term that refers to data that has been collected, organized, and isolated so that it can be used for reporting, analytics, and also for archival purposes. \n",
    "\n",
    "The different types of Data Repositories include:\n",
    "\n",
    "Databases, which can be relational or non-relational, each following a set of organizational principles, the types of data they can store, and the tools that can be used to query, organize, and retrieve data.\n",
    "\n",
    "Data Warehouses, that consolidate incoming data into one comprehensive store house. \n",
    "\n",
    "Data Marts, that are essentially sub-sections of a data warehouse, built to isolate data for a particular business function or use case.\n",
    "\n",
    "Data Lakes, that serve as storage repositories for large amounts of structured, semi-structured, and unstructured data in their native format.\n",
    "\n",
    "Big Data Stores, that provide distributed computational and storage infrastructure to store, scale, and process very large data sets.\n",
    "\n",
    "The ETL, or Extract Transform and Load, Process is an automated process that converts raw data into analysis-ready data by:\n",
    "\n",
    "Extracting data from source locations.\n",
    "\n",
    "Transforming raw data by cleaning, enriching, standardizing, and validating it.\n",
    "\n",
    "Loading the processed data into a destination system or data repository.\n",
    "\n",
    "The ELT, or Extract Load and Transfer, Process is a variation of the ETL Process. In this process, extracted data is loaded into the target system before the transformations are applied. This process is ideal for Data Lakes and working with Big Data.\n",
    "\n",
    "Data Pipeline, sometimes used interchangeably with ETL and ELT, encompasses the entire journey of moving data from its source to a destination data lake or application, using the ETL or ELT process.\n",
    "\n",
    "Data Integration Platforms combine disparate sources of data, physically or logically, to provide a unified view of the data for analytics purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef578e8-f0f3-4882-b080-d0ebc6164bd3",
   "metadata": {},
   "source": [
    "## Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f33d91-7ae6-4b61-8a5f-3ad1c390ad6f",
   "metadata": {},
   "source": [
    "### Hadoop, Kafka, and Spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc515202-11ad-4fcd-a6dd-1d2f54eed03c",
   "metadata": {},
   "source": [
    "In the big data ecosystem, Hadoop, Kafka, and Spark each play distinct but complementary roles, forming a powerful triad for managing and analyzing massive amounts of data. Hadoop provides the foundational layer for storing data in a distributed and fault-tolerant manner through its HDFS (Hadoop Distributed File System), and it originally handled processing via MapReduce, though that’s now often replaced with more modern tools like Spark. Kafka, on the other hand, is a high-throughput, distributed messaging system used to stream real-time data between systems — for instance, capturing user actions, application logs, or events from various sources and publishing them in real-time pipelines. Kafka itself doesn’t process or store data long-term, but it’s excellent for reliably transporting data. This is where Apache Spark enters the picture: it acts as the computational engine that can ingest data from both Hadoop (for batch processing) and Kafka (for stream processing), then transform, analyze, and output results — all at scale and often in-memory, which makes it much faster than older MapReduce models. Spark not only handles both batch and real-time data workflows efficiently, but it also supports advanced analytics like SQL queries, machine learning, and graph computation — making it the brain that connects and processes the data Kafka transports and Hadoop stores. Together, these tools provide an end-to-end pipeline: Kafka moves data, Hadoop stores it, and Spark analyzes and transforms it — whether in real-time or in scheduled jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fdca8c-c9bd-4f26-a4b4-3b26437e9b99",
   "metadata": {},
   "source": [
    "### Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccdb3adc-a959-4a90-99be-e65d8f67908e",
   "metadata": {},
   "source": [
    "The Big Data processing technologies provide\n",
    "ways to work with large sets of structured, semi-structured, and unstructured data so\n",
    "that value can be derived from big data. In some of the other videos, we discussed\n",
    "Big Data technologies such as NoSQL databases and Data Lakes. In this video, we are going to talk about\n",
    "three open source technologies and the role they play in big data analytics — ApacheHadoop,\n",
    "Apache Hive, and Apache Spark. Hadoop is a collection of tools that provides\n",
    "distributed storage and processing of big data. Hive is a data warehouse for data query and\n",
    "analysis built on top of Hadoop. Spark is a distributed data analytics framework\n",
    "designed to perform complex data analytics in real-time. Hadoop, a java-based open-source framework,\n",
    "allows distributed storage and processing of large datasets across clusters of computers. In Hadoop distributed system, a node is a\n",
    "single computer, and a collection of nodes forms a cluster. Hadoop can scale up from a single node to\n",
    "any number of nodes, each offering local storage and computation. Hadoop provides a reliable, scalable, and\n",
    "cost-effective solution for storing data with no format requirements. Using Hadoop, you can: Incorporate emerging data formats, such as\n",
    "streaming audio, video, social media sentiment, and clickstream data, along with structured,\n",
    "semi-structured, and unstructured data not traditionally used in a data warehouse. Provide real-time, self-service access for\n",
    "all stakeholders. Optimize and streamline costs in your enterprise\n",
    "data warehouse by consolidating data across the organization and moving “cold” data,\n",
    "that is, data that is not in frequent use, to a Hadoop-based system. One of the four main components of Hadoop\n",
    "is Hadoop Distributed File System, or HDFS, which is a storage system for big data that\n",
    "runs on multiple commodity hardware connected through a network. HDFS provides scalable and reliable big data\n",
    "storage by partitioning files over multiple nodes. It splits large files across multiple computers,\n",
    "allowing parallel access to them. Computations can, therefore, run in parallel\n",
    "on each node where data is stored. It also replicates file blocks on different\n",
    "nodes to prevent data loss, making it fault-tolerant. Let’s understand this through an example. Consider a file that includes phone numbers\n",
    "for everyone in the United States; the numbers for people with last name starting with A\n",
    "might be stored on server 1, B on server 2, and so on. With Hadoop, pieces of this phonebook would\n",
    "be stored across the cluster. To reconstruct the entire phonebook, your\n",
    "program would need the blocks from every server in the cluster. HDFS also replicates these smaller pieces\n",
    "onto two additional servers by default, ensuring availability when a server fails, In addition to higher availability, this offers\n",
    "multiple benefits. It allows the Hadoop cluster to break up work\n",
    "into smaller chunks and run those jobs on all servers in the cluster for better scalability. Finally, you gain the benefit of data locality,\n",
    "which is the process of moving the computation closer to the node on which the data resides. This is critical when working with large data\n",
    "sets because it minimizes network congestion and increases throughput. Some of the other benefits that come from\n",
    "using HDFS include: Fast recovery from hardware failures, because\n",
    "HDFS is built to detect faults and automatically recover. Access to streaming data, because HDFS supports\n",
    "high data throughput rates. Accommodation of large data sets, because\n",
    "HDFS can scale to hundreds of nodes, or computers, in a single cluster. Portability, because HDFS is portable across\n",
    "multiple hardware platforms and compatible with a variety of underlying operating systems. Hive is an open-source data warehouse software\n",
    "for reading, writing, and managing large data set files that are stored directly in either\n",
    "HDFS or other data storage systems such as Apache HBase. Hadoop is intended for long sequential scans\n",
    "and, because Hive is based on Hadoop, queries have very high latency—which means Hive\n",
    "is less appropriate for applications that need very fast response times. Also, Hive is read-based, and therefore not\n",
    "suitable for transaction processing that typically involves a high percentage of write operations. Hive is better suited for data warehousing\n",
    "tasks such as ETL, reporting, and data analysis and includes tools that enable easy access\n",
    "to data via SQL. This brings us to Spark, a general-purpose\n",
    "data processing engine designed to extract and process large volumes of data for a wide\n",
    "range of applications, including Interactive Analytics, Streams Processing, Machine Learning,\n",
    "Data Integration, and ETL. It takes advantage of in-memory processing\n",
    "to significantly increase the speed of computations and spilling to disk only when memory is constrained. Spark has interfaces for major programming\n",
    "languages, including Java, Scala, Python, R, and SQL. It can run using its standalone clustering\n",
    "technology as well as on top of other infrastructures such as Hadoop. And it can access data in a large variety\n",
    "of data sources, including HDFS and Hive, making it highly versatile. The ability to process streaming data fast\n",
    "and perform complex analytics in real-time is the key use case for Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9320f0-d647-4287-81cd-69773de1788a",
   "metadata": {},
   "source": [
    "### Summery"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0c21bd7-ff44-4e32-86fa-5dfcc777075b",
   "metadata": {},
   "source": [
    "In this lesson, you have learned:\n",
    "\n",
    "Big Data refers to the vast amounts of data that is being produced each moment of every day, by people, tools, and machines. The sheer velocity, volume, and variety of data challenged the tools and systems used for conventional data, leading to the emergence of processing tools and platforms designed specifically for Big Data.\n",
    "\n",
    "Big Data processing technologies help derive value from big data. These include NoSQL databases and Data Lakes and open-source technologies such as Apache Hadoop, Apache Hive, and Apache Spark.\n",
    "\n",
    "Hadoop provides distributed storage and processing of large datasets across clusters of computers. One of its main components, the Hadoop File Distribution System, or HDFS, is a storage system for big data.\n",
    "\n",
    "Hive is a data warehouse software for reading, writing, and managing large datasets.\n",
    "\n",
    "Spark is a general-purpose data processing engine designed to extract and process large volumes of data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e23ed-7266-464f-9e21-c06c6173cade",
   "metadata": {},
   "source": [
    "## Data Platforms, Data Stores, and Security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64697e82-f843-449f-8fb9-8cb21a2f2c0b",
   "metadata": {},
   "source": [
    "### ETL and Integration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b59c9f12-626c-486c-a99f-91e40e58e702",
   "metadata": {},
   "source": [
    "ETL can include data integration, but it can also happen from one source — so no integration is needed\n",
    "\n",
    "Data integration is often a subset of ETL, but not always.\n",
    "\n",
    "In real-world data architecture:\n",
    "\n",
    "Sometimes integration happens via ETL\n",
    "\n",
    "Sometimes integration happens upstream\n",
    "\n",
    "Sometimes integration happens downstream (e.g., in analytics tools or dashboards!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4583f-c7bb-4be2-aaac-01232306cab4",
   "metadata": {},
   "source": [
    "### Architecting the Data Platform"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13d7e5d3-8c6c-4830-a22a-ed6863fba924",
   "metadata": {},
   "source": [
    "In this video, we will learn about the layers\n",
    "of a data platform architecture. A layer represents functional components that\n",
    "perform a specific set of tasks in the data platform. The layers that we’re going to explore,\n",
    "include: Data Ingestion or Data Collection Layer, Data Storage and Integration Layer, Data Processing Layer, and Analysis and User Interface Layer. We will also learn about the Data Pipeline\n",
    "Layer, which overlays multiple layers. The Data Collection Layer is responsible for\n",
    "connecting to the source systems and bringing the data from these systems into the data\n",
    "platform. This layer performs the following key tasks: Connect to data sources. Transfer data from these data sources to the\n",
    "data platform in streaming, batch, or both modes. Maintain information about the data collected\n",
    "in the metadata repository. For example, how much data was ingested in\n",
    "a batch, data source, and other descriptive information. Google Cloud DataFlow, IBM Streams, IBM Streaming\n",
    "Analytics on Cloud, Amazon Kinesis, and Apache Kafka are some of the tools used for data\n",
    "ingestion, supporting both batch and streaming modes. Once data is ingested, it needs to be stored\n",
    "and integrated. The Storage and Integration layer in a data\n",
    "platform needs to: Store data for processing and long-term use. Transform and merge extracted data, either\n",
    "logically or physically. Make data available for processing in both\n",
    "streaming and batch modes. The storage layer needs to be reliable, scalable,\n",
    "high-performing, and also cost-efficient. IBM DB2, Microsoft SQL Server, MySQL, Oracle\n",
    "Database, and PostgreSQL are some of the popular relational databases. Cloud-based relational databases, also referred\n",
    "to as Database-as-a-Service, have gained great popularity over the recent years. Such as IBM DB2 on Cloud, Amazon Relational\n",
    "Database Service (RDS), and Google Cloud SQL, and SQL Azure. In the NoSQL, or non-relational database systems\n",
    "on the cloud, we have IBM Cloudant, Redis, MongoDB, Cassandra, and Neo4J. Tools for integration include IBM’s Cloud\n",
    "Pak for Data and Cloud Pak for Integration; Talend’s Data Fabric and Open Studio. Open-source tools such as Dell Boomi and SnapLogic\n",
    "are also very popular integration tools. There are a number of vendors offering cloud-based\n",
    "Integration Platform as a Service (or iPaaS). For example, Adeptia Integration Suite, Google\n",
    "Cloud's Cooperation 534, IBM's Application Integration Suite on Cloud, and Informatica's\n",
    "Integration Cloud. Once the data has been ingested, stored, and\n",
    "integrated, it needs to be processed. Data validations, transformations, and applying\n",
    "business logic to the data are some of the things that need to happen in this layer. The processing layer should be able to: Read data in batch or streaming modes from\n",
    "storage and apply transformations. Support popular querying tools and programming\n",
    "languages. Scale to meet the processing demands of a\n",
    "growing dataset. Provide a way for analysts and data scientists\n",
    "to work with data in the data platform. Some of the transformation tasks that occur\n",
    "in this layer include: Structuring, essentially, actions that change\n",
    "the form and schema of the data. This change may be as simple as changing the\n",
    "order of fields within a record or dataset or as complex as combining fields into complex\n",
    "structures using joins and unions. Normalization, which focuses on cleaning the\n",
    "database of unused data and reducing redundancy and inconsistency. Denormalization, which combines data from\n",
    "multiple tables into a single table so that it can be queried more efficiently for reporting\n",
    "and analysis. And Data Cleaning, which fixes irregularities\n",
    "in data to provide credible data for downstream applications and uses. There are a host of tools available for performing\n",
    "these transformations on data, selected based on the data size, structure, and specific\n",
    "capabilities of the tool. Such as spreadsheets, OpenRefine, Google DataPrep,\n",
    "Watson Studio Refinery, and Trifacta Wrangler. Python and R also offer several libraries\n",
    "and packages that are explicitly created for processing data. It’s important to note that storage and\n",
    "processing may not always be performed in separate layers. For example, in relational databases, storage\n",
    "and processing can occur in the same layer, while in Big Data systems, data can be first\n",
    "stored in the Hadoop File Distribution System, or HDFS, and then processed in a data processing\n",
    "engine like Spark. And, the data processing layer can also precede\n",
    "the data storage layer, where transformations are applied before the data is loaded, or\n",
    "stored, in the database. The Analysis and User Interface Layer delivers\n",
    "processed data to data consumers. Data consumers can include: Business Intelligence Analysts and business\n",
    "stakeholders who consume this data through interactive visual representations, such as\n",
    "dashboards and analytical reports. Data Scientists and Data Analytics that further\n",
    "process this data for specific use cases. Other applications and services that may need\n",
    "this data as input for further use. The Analysis and UI Layer needs to support: Querying tools and programming languages. For example, SQL for querying relational databases and\n",
    "SQL-like querying tools for non-relational databases, such as CQL for Cassandra, Programming languages such as Python, R, and\n",
    "Java, APIs that can be used to run reports on data\n",
    "for both online and offline processing. APIs that can consume data from the storage\n",
    "in real-time for use in other applications and services. Dashboarding and Business Intelligence applications. For example, IBM Cognos Analytics, Tableau,\n",
    "Jupyter Notebooks, Python and R libraries, and Microsoft Power BI. Overlaying the Data Ingestion, Data Storage\n",
    "and Integration, and Data Processing layers is the Data Pipeline layer with the Extract,\n",
    "Transform, and Load tools. This layer is responsible for implementing\n",
    "and maintaining a continuously flowing data pipeline. There are a number of data pipeline solutions\n",
    "available, most popular among them being Apache Airflow and DataFlow. In this video, you learned about the layers\n",
    "of a data platform architecture. This is a simplified rendition of a complex\n",
    "architecture that supports a broad spectrum of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5403c80a-3528-41bb-b883-070c138f3959",
   "metadata": {},
   "source": [
    "### Factors for Selecting and Designing Data Stores"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93c1f692-6a76-4bae-8e3f-062db1b7bd98",
   "metadata": {},
   "source": [
    "A data store, or data repository, is a general\n",
    "term used to refer to data that has been collected, organized, and isolated so that it can be\n",
    "used for business operations or mined for reporting and data analysis. A repository can be a database, data warehouse,\n",
    "data mart, big data store, or a data lake. A well-designed data repository is essential\n",
    "for building a system that is scalable and capable of performing during high workloads. In this video, we will look at some of the\n",
    "primary considerations for designing a data store, such as: The type of data you want to store Volume of data Intended use of data Storage considerations And Privacy, security, and governance needs of\n",
    "your organization. There are multiple types of databases and\n",
    "selecting the right one is a crucial part of designing. A database is essentially a collection of\n",
    "data designed for the input, storage, search and retrieval, and modification of data. Depending on the type of data, databases can\n",
    "be categorized in two primary ways – relational and non-relational. Relational databases, or RDBMSes, are best\n",
    "used for structured data, which has a well-defined schema and can be organized into a tabular\n",
    "format. Non-relational databases, or NoSQL, are best\n",
    "for semi-structured and unstructured data, that is, schema-less and free-form data. Non-relational databases, based on the type\n",
    "of data and how you want to query the data, are of four different types—key-value, document,\n",
    "column, and graph-based. If you're looking to run complex search queries\n",
    "and multi-operation transactions, for example, a document-based database may not be the best\n",
    "option for you. Like you would not opt for a graph-based database\n",
    "if you need to process high-volume transactions because graph-based databases are not optimized\n",
    "for large-volume analytics queries. Another important consideration that goes\n",
    "into the design of a data store is the volume, or scale, of data. When you require to store large volumes of\n",
    "raw data in its native format, straight from its source, a data lake would be the appropriate\n",
    "choice for you. With a data lake, you can store both relational\n",
    "and non-relational data at scale without defining the data's structure and schema. Or when you're dealing with Big Data, that\n",
    "is data that is not only high-volume but also high-velocity, of diverse types, and needs\n",
    "distributed processing for fast analytics, then a big data repository would be an option\n",
    "you would explore. Big data stores split large files across multiple\n",
    "computers allowing parallel access to them. Computations run in parallel on each node\n",
    "where data is stored. How you intend to use the data you are collecting\n",
    "is also an important consideration for the choice and design of a data store. The number of transactions, frequency of updates,\n",
    "type of operations performed on the data, response time, and backup and recovery requirements\n",
    "all need to be provisioned for in the design of a data store. Whether you need to use the data store for\n",
    "recording high-volume transactional data or executing complex queries for analytical purposes,\n",
    "your processing and storage needs will differ. Transactional systems, that is systems used\n",
    "for capturing high-volume transactions, need to be designed for high-speed read, write,\n",
    "and update operations. Analytical systems, on the other hand, need\n",
    "complex queries to be applied to large amounts of historical data aggregated from transactional\n",
    "systems. They need faster response times to complex\n",
    "queries. Schema design, indexing, and partitioning\n",
    "strategies have a big role to play in performance of systems based on how data is getting used. The intended use of data also drives scalability\n",
    "as a design consideration. Scalability of a data store is its capability\n",
    "to handle growth in the amount of data, workloads, and users. Normalization of the database is another important\n",
    "consideration at the design stage. Normalization is the process of efficiently\n",
    "organizing data in a database. Done right, it helps in the optimal use of\n",
    "storage space, makes database maintenance easier, and provides faster access to data. Normalization is important for systems that\n",
    "handle transactional data. But for systems designed for handling analytical\n",
    "queries, normalization can lead to performance issues. Now we'll look at some key design considerations\n",
    "from the perspective of storage. These are Performance, Availability, Integrity,\n",
    "and Recoverability of Data. Performance parameters include throughput\n",
    "and latency. That is, the rate at which information can\n",
    "be read from and written to the storage and the time it takes to access a specific location\n",
    "in storage. Availability - Your storage solution must\n",
    "enable you to access your data when you need it, without exception. There should be no downtime. Integrity - Your data must be safe from corruption,\n",
    "loss, and outside attack. And Recoverability - Your storage solution should\n",
    "ensure that you can recover your data in the event of failures and natural disasters. A secure data strategy is a layered approach. It includes access control, multizone encryption,\n",
    "data management, and monitoring systems. Regulations such as GDPR, CCPA, and HIPAA\n",
    "restrict the ownership, use, and management of personal and sensitive data. Data needs to be made available through controlled\n",
    "data flow and data management by using multiple data protection techniques. This is an important part of a data store\n",
    "design. Strategies for data privacy, security, and\n",
    "governance regulations need to be a of a data store's design from the start. Done at a later stage it results in patchwork. In this video, we learned about some of the\n",
    "considerations that guide the design of a data store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46e89c-4f0d-4ff9-acc6-3c1ca53407b7",
   "metadata": {},
   "source": [
    "### Security"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04fabf5a-17e8-4650-a296-4b5d489a3b12",
   "metadata": {},
   "source": [
    "[Music] enterprise level data platforms and data repositories need to tackle security at multiple levels in this video we will learn about the various facets of security as it applies to data platforms and data life cycle management these include physical infrastructure security network security application security and data security let us first look at the three key components to creating an effective strategy for information security in general these are popularly referred to as the cia triad the c in this triad stands for confidentiality through controlling unauthorized access i for integrity through validating that your resources are trustworthy and have not been tampered with and a is for availability by ensuring authorized users have access to resources when they need it the cia triad is applicable to all facets of security be it infrastructure network application or data security now we'll look at the four different facets or levels of security the first level is physical infrastructure security a key component of security for an it system is the security of the physical infrastructure and facilities that house the system in the case of cloud computing this extends to the cloud service providers infrastructure and facilities here are some of the measures that are taken to ensure physical infrastructure security access to the perimeter of the facility based on authentication and round-the-clock surveillance for entry and exit points multiple power feeds from independent utility providers with dedicated generators and ups battery backup heating and cooling mechanisms for managing the temperature and humidity levels in the facility and by factoring in environmental threats before considering the location of the facility for example infrastructure facilities are never housed in flood plains in areas prone to earthquakes the infrastructure is housed in an earthquake-resistant structure multi-level lightning protection and earthing systems are also installed in such facilities the next level is network security network security is vital to keep interconnected systems and data safe network security solutions include firewalls to prevent unauthorized access to private networks that are connected to the internet network access control to ensure endpoint security by allowing only authorized devices to connect to the network for example a corporate network may not allow devices with outdated service packs to connect to their network network segmentation to create silos or virtual local area networks within a network so that you can segregate your assets into individual silos based on the level of security required for different assets security protocols to ensure attackers cannot tap into data while it is in transit and intrusion detection and intrusion prevention systems that inspect incoming traffic for intrusion attempts and vulnerabilities the third dimension of security is application security application security is critical for keeping customer data private and ensuring applications are fast and responsive security needs to be built into the foundation of an application in order to prevent other applications and services from introducing vulnerabilities you can make your application safe by following security engineering practices such as threat modeling to identify relative weaknesses and attack patterns related to the application secure design that mitigates risks secure coding guides and practices that prevent vulnerabilities and security testing to fix problems before the application is deployed and to validate that it is free from known security issues and the fourth and final dimension is data security data is either at rest in storage or in transit between systems applications services and workloads be it at rest or in motion data needs to be protected one of the primary controls for data security is to enable access to data through a system of authentication and authorization authentication systems verify that you are who you say you are and they accomplish this using passwords tokens biometrics or a combination of these authorization ensures that users access information based on their roles and the privileges assigned to their roles data at rest includes files objects and storage this type of data is stored physically such as in a database data warehouse tapes off-site backups or on mobile devices organizations can use encryption to fight threats to their data at rest encrypting data protects information from disclosure even if that information is lost or intercepted data that is moving from one place to another such as when it is transmitted over the internet is referred to as data in transit or data in motion encryption methods such as https ssl and tls are often used to protect data in motion later on in the course you will learn more about the different stages of data life cycle the vulnerabilities data can be exposed to and the security features that help protect data end to end through its life cycle it is vital to proactively monitor track and react to security violations in time and for that you need end-to-end visibility and integration of security processes and tools throughout the enterprise security monitoring and intelligence systems create a complete audit history for triage and compliance purposes and provide reports and alerts that help enterprises react to security violations in time in this video you learned about some of the facets of security as it applies to the enterprise level every enterprise needs to have a corporate level security policy that ensures business i.t and all stakeholders contribute to achieving security goals through people policy processes systems and tools [Music]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56999a0-edc6-4f32-88af-8d6c1252f62f",
   "metadata": {},
   "source": [
    "### Summary and Highlights"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b42e0135-a04b-4b15-9712-b9591a2f113f",
   "metadata": {},
   "source": [
    "The architecture of a data platform can be seen as a set of layers, or functional components, each one performing a set of specific tasks. These layers include:\n",
    "\n",
    "Data Ingestion or Data Collection Layer, responsible for bringing data from source systems into the data platform.\n",
    "\n",
    "Data Storage and Integration Layer, responsible for storing and merging extracted data.\n",
    "\n",
    "Data Processing Layer, responsible for validating, transforming, and applying business rules to data.\n",
    "\n",
    "Analysis and User Interface Layer, responsible for delivering processed data to data consumers.\n",
    "\n",
    "Data Pipeline Layer, responsible for implementing and maintaining a continuously flowing data pipeline.\n",
    "\n",
    "A well-designed data repository is essential for building a system that is scalable and capable of performing during high workloads. \n",
    "\n",
    "The choice or design of a data store is influenced by the type and volume of data that needs to be stored, the intended use of data, and storage considerations. The privacy, security, and governance needs of your organization also influence this choice.\n",
    "\n",
    "The CIA, or Confidentiality, Integrity, and Availability triad are three key components of an effective strategy for information security. The CIA triad is applicable to all facets of security, be it infrastructure, network, application, or data security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3d90f-1f92-42c0-a18a-0ed1113b10e9",
   "metadata": {},
   "source": [
    "## Data Collection and Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b00f58-cf22-4df3-9000-08a22f6b1cc2",
   "metadata": {},
   "source": [
    "### How to Gather and Import Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20ba7002-7006-45fe-b6b2-c3d9f904769a",
   "metadata": {},
   "source": [
    "In this video, we will learn about the different\n",
    "methods and tools available for gathering data from the data sources discussed earlier\n",
    "in the course—such as databases, the web, sensor data, data exchanges, and several other\n",
    "sources leveraged for specific data needs. We will also learn about importing data into\n",
    "different types of data repositories. SQL, or Structured Query Language, is a querying\n",
    "language used for extracting information from relational databases. SQL offers simple commands to specify what\n",
    "is to be retrieved from the database, the table from which it needs to be extracted,\n",
    "grouping records with matching values, dictating the sequence in which the query results are\n",
    "displayed, and limiting the number of results that can be returned by the query, amongst\n",
    "a host of other features and functionalities. Non-relational databases can be queried using\n",
    "SQL or SQL-like query tools. Some non-relational databases come with their\n",
    "own querying tools such as CQL for Cassandra and GraphQL for Neo4J. Application Programming Interfaces (or APIs)\n",
    "are also popularly used for extracting data from a variety of data sources. APIs are invoked from applications that require\n",
    "the data and access an end-point containing the data. End-points can include databases, web services,\n",
    "and data marketplaces. APIs are also used for data validation. For example, a data analyst may utilize an\n",
    "API to validate postal addresses and zip codes. Web scraping, also known as screen scraping\n",
    "or web harvesting, is used for downloading specific data from web pages based on defined\n",
    "parameters. Among other things, web scraping is used to\n",
    "extract data such as text, contact information, images, videos, podcasts, and product items\n",
    "from a web property. RSS feeds are another source typically used\n",
    "for capturing updated data from online forums and news sites where data is refreshed on\n",
    "an ongoing basis. Data streams are a popular source for aggregating\n",
    "constant streams of data flowing from sources such as instruments, IoT devices and applications,\n",
    "and GPS data from cars. Data streams and feeds are also used for extracting\n",
    "data from social media sites and interactive platforms. Data Exchange platforms allow the exchange\n",
    "of data between data providers and data consumers. Data Exchanges have a set of well-defined\n",
    "exchange standards, protocols, and formats relevant for exchanging data. These platforms not only facilitate the exchange\n",
    "of data, they also ensure that security and governance are maintained. They provide data licensing workflows, de-identification\n",
    "and protection of personal information, legal frameworks, and a quarantined analytics environment. Examples of popular data exchange platforms\n",
    "include AWS Data Exchange, Crunchbase, Lotame, and Snowflake. Numerous other data sources can be tapped\n",
    "into for specific data needs. For marketing trends and ad spending, for\n",
    "example, research firms like Forrester and Business Insider are known to provide reliable\n",
    "data. Research and advisory firms such as Gartner\n",
    "and Forrester are widely trusted sources for strategic and operational guidance. Similarly, there are many trusted names in\n",
    "the areas of user behavior data, mobile and web usage, market surveys, and demographic\n",
    "studies. Data that has been identified and gathered\n",
    "from the various data sources now needs to be loaded or imported into a data repository\n",
    "before it can be wrangled, mined, and analyzed. The importing process involves combining data\n",
    "from different sources to provide a combined view and a single interface using which you\n",
    "can query and manipulate the data. Depending on the data type, the volume of\n",
    "data, and the type of destination repository, you may need varying tools and methods. Specific data repositories are optimized for\n",
    "certain types of data. Relational databases store structured data\n",
    "with a well-defined schema. If you’re using a relational database as\n",
    "the destination system, you will only be able to store structured data, such as data from\n",
    "OLTP systems, spreadsheets, online forms, sensors, network and web logs. Structured data can also be stored in NoSQL. Semi-structured data is data that has some\n",
    "organizational properties but not a rigid schema, such as, data from emails, XML, zipped\n",
    "files, binary executables, and TCP/IP protocols. Semi-structured can be stored in NoSQL clusters. XML and JSON are commonly used for storing\n",
    "and exchanging semi-structured data. JSON is also the preferred data type for web\n",
    "services. Unstructured data is data that does not have\n",
    "a structure and cannot be organized into a schema, such as data from web pages, social\n",
    "media feeds, images, videos, documents, media logs, and surveys. NoSQL databases and Data Lakes provide a good\n",
    "option to store and manipulate large volumes of unstructured data. Data lakes can accommodate all data types\n",
    "and schema. ETL tools and data pipelines provide automated\n",
    "functions that facilitate the process of importing data. Tools such as Talend and Informatica, and\n",
    "programming languages such as Python and R, and their libraries, are widely used for importing\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750be4f9-aca5-4ee3-81ff-741a1a548954",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb4e2c0a-cb2d-4ca0-859a-e153c781d943",
   "metadata": {},
   "source": [
    "Raw data has to undergo a series of transformations\n",
    "and cleansing activities in order to be analytics-ready. Data wrangling, also known as data munging,\n",
    "is an iterative process that involves data exploration, transformation, validation, and\n",
    "making data available for a credible and meaningful analysis. It includes a whole range of transformations\n",
    "and cleansing activities, some of which we will learn about in this video. Now let’s look at some of key transformations\n",
    "that data typically undergoes in a data analytics scenario. Let’s begin with the first transformation\n",
    "task –(1) Structuring. This task includes actions that change the\n",
    "form and schema of your data. The incoming data can be in varied formats. You might, for example, have some data coming\n",
    "from a relational database and some data from Web APIs. In order to merge them, you will need to change\n",
    "the form or schema of your data. This change may be as simple as (1-1)changing the\n",
    "order of fields within a record or dataset or as complex as (1-2)combining fields into complex\n",
    "structures. (1-2-1)Joins and (1-2-2)Unions are the most common structural\n",
    "transformations used to combine data from one or more tables. How they combine the data is different. Joins combine columns. When two tables are joined together, columns\n",
    "from the first source table are combined with columns from the second source table—in\n",
    "the same row. So, each row in the resultant table contains\n",
    "columns from both tables. Unions combine rows. Rows of data from the first source table are\n",
    "combined with rows of data from the second source table into a single table. Each row in the resultant table is from one\n",
    "source table or another. Transformation can also include (2)normalization\n",
    "and (3)denormalization of data. Normalization focuses on cleaning the database\n",
    "of unused data and reducing redundancy and inconsistency. Data coming from transactional systems, for\n",
    "example, where a number of insert, update, and delete operations are performed on an\n",
    "ongoing basis, are highly normalized. Denormalization is used to combine data from\n",
    "multiple tables into a single table so that it can be queried faster. For example, normalized data coming from transactional\n",
    "systems is typically denormalized before running queries for reporting and analysis. Another transformation type is (4)Cleaning. Cleaning tasks are actions that fix irregularities\n",
    "in data in order to produce a credible and accurate analysis. The first step in the data cleaning workflow\n",
    "is to (4-1)detect the different types of issues and errors that your dataset may have. You can use (4-1-1)scripts and tools that allow you\n",
    "to define specific rules and constraints and validate your data against these rules and\n",
    "constraints. You can also use (4-1-2)data profiling and (4-1-3)data visualization\n",
    "tools for inspection. Data profiling helps you to inspect the source\n",
    "data to understand the structure, content, and interrelationships in your data. It uncovers anomalies and data quality issues. For example, blank or null values, duplicate\n",
    "data, or whether the value of a field falls within the expected range. Visualizing the data using statistical methods\n",
    "can help you to spot outliers. For example, plotting the average income in\n",
    "a demographic dataset can help you spot outliers. That brings us to (4-2)the actual cleaning of the\n",
    "data. The techniques you apply for cleaning your\n",
    "dataset will depend on your use case and the type of issues you encounter. Let’s look at some of the more common data\n",
    "issues. Let’s start with (problem1)missing values. Missing values are very important to deal\n",
    "with as they can cause unexpected or biased results. You can choose to (problem1-solution1)filter out the records with\n",
    "missing values or (problem1-solution2)find a way to source that information in case it is intrinsic to your\n",
    "use case. For example, missing age data from a demographics\n",
    "study. A third option is a method known as (problem1-solution3)imputation,\n",
    "which calculates the missing value based on statistical values. Your decision on the course of action you\n",
    "choose needs to be anchored in what’s best for your use case. You may also come across (problem2) duplicate data, data\n",
    "points that are repeated in your dataset. These need to (problem2-solution)be removed. Another type of issue you may encounter is\n",
    "that of irrelevant data. Data that does not fit within the context\n",
    "of your use case can be considered (problem3)irrelevant data. For example, if you are analyzing data about\n",
    "the general health of a segment of the population, their contact numbers may not be relevant\n",
    "for you. Cleaning can involve (problem4) data type conversion\n",
    "as well. This is needed to ensure that values in a\n",
    "field are stored as the data type of that field—for example, numbers stored as numerical\n",
    "data type or date stored as a date data type. You may also need to clean your data in order\n",
    "to (problem5)standardize it. For example, for strings, you may want all\n",
    "values to be in (problem5-example1)lower case. Similarly, (problem5-example2)date formats and units of measurement\n",
    "need to be standardized. Then there are (problem5-example3)syntax errors. For example, white spaces, or extra spaces\n",
    "at the beginning or end of a string is a syntax error that needs to be rectified. This can also include fixing typos or format,\n",
    "for example, the state name being entered as a full form such as New York versus an\n",
    "abbreviated form such as NY in some records. Data can also have outliers, or values that\n",
    "are vastly different from other observations in the dataset. (problem6)Outliers (problem6-1)may, or (problem6-2)may not, be incorrect. For example, when an age field in a voters\n",
    "database has the value 5, you know it is incorrect data and needs to be corrected. Now let’s consider a group of people where\n",
    "the annual income is in the range of one hundred thousand to two hundred thousand dollars—except\n",
    "for that one person who earns a million dollars a year. While this data point is not incorrect, it\n",
    "is an outlier, and needs to be looked at. In this video, you learned about some of the\n",
    "transformations and cleansing activities that need to be performed on raw data in order\n",
    "to make it analytics-ready. You also learned how data profiling and data\n",
    "visualization help spot the issues that need to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce56a8-0224-43dc-8d81-340359250611",
   "metadata": {},
   "source": [
    "### Tools for Data Wrangling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "272e0275-a3cc-4752-8147-d0cecb364ee6",
   "metadata": {},
   "source": [
    "In this video, we will look at some of the\n",
    "popularly used data wrangling software and tools, such as: Excel Power Query / Spreadsheets and Add-ins OpenRefine Google DataPrep Watson Studio Refinery Trifacta Wrangler Python R Let’s begin with the most basic software\n",
    "used for manual wrangling—Spreadsheets. Spreadsheets such as Microsoft Excel and Google\n",
    "Sheets have a host of features and in-built formulae that can help you identify issues,\n",
    "clean, and transform data. Add-ins are available that allow you to import\n",
    "data from several different types of sources and clean and transform data as needed—such\n",
    "as Microsoft Power Query for Excel and Google Sheets Query function for Google Sheets. OpenRefine is an open-source tool that allows\n",
    "you to import and export data in a wide variety of formats, such as TSV, CSV, XLS, XML, and\n",
    "JSON. Using OpenRefine, you can clean data, transform\n",
    "it from one format to another, and extend data with web services and external data. OpenRefine is easy to learn and easy to use. It offers menu-based operations, which means\n",
    "you don’t need to memorize commands or syntax. Google DataPrep is an intelligent cloud data\n",
    "service that allows you to visually explore, clean, and prepare both structured and unstructured\n",
    "data for analysis. It is a fully managed service, which means\n",
    "you don’t need to install or manage the software or the infrastructure. DataPrep is extremely easy to use. With every action that you take, you get suggestions\n",
    "on what your ideal next step should be. DataPrep can automatically detect schemas,\n",
    "data types, and anomalies. Watson StudioIBM Data Refinery, available\n",
    "via IBM Watson Studio or Cloud Pak for Data, allows you to discover, cleanse, and transform\n",
    "data with built-in operations. It transforms large amounts of raw data into\n",
    "consumable, quality information that’s ready for analytics. Data Refinery offers the flexibility of exploring\n",
    "data residing in a spectrum of data sources. It detects data types and classifications\n",
    "automatically and also enforces applicable data governance policies automatically. Trifacta Wrangler is an interactive cloud-based\n",
    "service for cleaning and transforming data. It takes messy, real-world data and cleans\n",
    "and rearranges it into data tables, which can then be exported to Excel, Tableau, and\n",
    "R. It is known for its collaboration features,\n",
    "allowing multiple team members to work simultaneously. Python has a huge library and set of packages\n",
    "that offer powerful data manipulation capabilities. Let’s look at a few of these libraries and\n",
    "packages. Jupyter Notebook is an open-source web application\n",
    "widely used for data cleaning and transformation, statistical modeling, also data visualization. Numpy, or Numerical Python, is the most basic\n",
    "package that Python offers. It is fast, versatile, interoperable, and\n",
    "easy to use. It provides support for large, multi-dimensional\n",
    "arrays and matrices, and high-level mathematical functions to operate on these arrays. Pandas is designed for fast and easy data\n",
    "analysis operations. It allows complex operations such as merging,\n",
    "joining, and transforming huge chunks of data, performed using simple, single-line commands. Using Pandas, you can prevent common errors\n",
    "that result from misaligned data coming in from different sources. R, too, offers a series of libraries and packages\n",
    "that are explicitly created for wrangling messy data—such as Dplyr, Data.table, and\n",
    "Jsonlite. Using these libraries, you can investigate,\n",
    "manipulate, and analyze data. Dplyr is a powerful library for data wrangling. It has a precise and straightforward syntax. Data.table helps to aggregate large data sets\n",
    "quickly. Jsonlite is a robust JSON parsing tool, great\n",
    "for interacting with web APIs. Tools for data wrangling come with varying\n",
    "capabilities and dimensions. Your decision regarding the best tool for\n",
    "your needs will depend on factors that are specific to your use case, infrastructure,\n",
    "and teams—such as supported data size, data structures, cleaning and transformation capabilities,\n",
    "infrastructure needs, ease of use, and learnability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21ce4d-9a68-4eb0-970a-02f08c2844db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
